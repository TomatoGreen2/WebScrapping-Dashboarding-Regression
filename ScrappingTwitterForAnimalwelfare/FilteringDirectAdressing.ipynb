{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a470f52-d7da-42d5-9b09-81fd162351df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### copied from: https://github.com/bocchilorenzo/ntscraper\n",
    "### modified nitter.py by adding function _get_tweet_replies\n",
    "### started issue in ntscraper git pages and provided this function which was included to repository\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from urllib.parse import unquote\n",
    "from time import sleep\n",
    "from base64 import b64decode\n",
    "from random import uniform\n",
    "from re import match, sub\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from logging.handlers import QueueHandler\n",
    "from multiprocessing import Pool, Queue, cpu_count\n",
    "from sys import stdout\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    "    datefmt=\"%d-%b-%y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(stdout)],\n",
    ")\n",
    "\n",
    "log_queue = Queue()\n",
    "log_handler = QueueHandler(log_queue)\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.addHandler(log_handler)\n",
    "\n",
    "valid_filters = [\n",
    "    \"nativeretweets\",\n",
    "    \"media\",\n",
    "    \"videos\",\n",
    "    \"news\",\n",
    "    \"verified\",\n",
    "    \"native_video\",\n",
    "    \"replies\",\n",
    "    \"links\",\n",
    "    \"images\",\n",
    "    \"safe\",\n",
    "    \"quote\",\n",
    "    \"pro_video\",\n",
    "]\n",
    "\n",
    "\n",
    "class Nitter:\n",
    "    def __init__(self, log_level=1, skip_instance_check=False):\n",
    "        \"\"\"\n",
    "        Nitter scraper\n",
    "\n",
    "        :param log_level: logging level\n",
    "        :param skip_instance_check: True if the health check of all instances and the instance change during execution should be skipped\n",
    "        \"\"\"\n",
    "        self.instances = self._get_instances()\n",
    "        if self.instances is None:\n",
    "            raise ValueError(\"Could not fetch instances\")\n",
    "        self.working_instances = []\n",
    "        self.skip_instance_check = skip_instance_check\n",
    "        if skip_instance_check:\n",
    "            self.working_instances = self.instances\n",
    "        else:\n",
    "            self._test_all_instances(\"/x\", no_print=True)\n",
    "        if log_level == 0:\n",
    "            log_level = logging.WARNING\n",
    "        elif log_level == 1:\n",
    "            log_level = logging.INFO\n",
    "        elif log_level:\n",
    "            raise ValueError(\"Invalid log level\")\n",
    "\n",
    "        logger = logging.getLogger()\n",
    "        logger.setLevel(log_level)\n",
    "\n",
    "        self.retry_count = 0\n",
    "        self.cooldown_count = 0\n",
    "        self.session_reset = False\n",
    "        self.instance = \"\"\n",
    "        self.r = None\n",
    "\n",
    "    def _initialize_session(self, instance):\n",
    "        \"\"\"\n",
    "        Initialize the requests session\n",
    "        \"\"\"\n",
    "        if instance is None:\n",
    "            if self.skip_instance_check:\n",
    "                raise ValueError(\"No instance specified and instance check skipped\")\n",
    "            self.instance = self.get_random_instance()\n",
    "            logging.info(\n",
    "                f\"No instance specified, using random instance {self.instance}\"\n",
    "            )\n",
    "        else:\n",
    "            self.instance = instance\n",
    "        self.r = requests.Session()\n",
    "        self.r.headers.update(\n",
    "            {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/118.0\",\n",
    "                \"Host\": self.instance.split(\"://\")[1],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def _is_instance_encrypted(self):\n",
    "        \"\"\"\n",
    "        Check if the current instance uses encrypted media\n",
    "\n",
    "        :return: True if encrypted, False otherwise\n",
    "        \"\"\"\n",
    "        soup = self._get_page(\"/x\")\n",
    "\n",
    "        if soup is None:\n",
    "            raise ValueError(\"Invalid instance\")\n",
    "\n",
    "        if (\n",
    "            soup.find(\"a\", class_=\"profile-card-avatar\").find(\"img\")\n",
    "            and \"/enc/\"\n",
    "            in soup.find(\"a\", class_=\"profile-card-avatar\").find(\"img\")[\"src\"]\n",
    "        ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _get_instances(self):\n",
    "        \"\"\"\n",
    "        Fetch the list of clear web Nitter instances from the wiki\n",
    "\n",
    "        :return: list of Nitter instances, or None if lookup failed\n",
    "        \"\"\"\n",
    "        r = requests.get(\"https://github.com/zedeus/nitter/wiki/Instances\")\n",
    "        instance_list = []\n",
    "        if r.ok:\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "            official = soup.find_all(\"tbody\")[0]\n",
    "            instance_list.append(official.find(\"a\")[\"href\"])\n",
    "            table = soup.find_all(\"tbody\")[2]\n",
    "            for instance in table.find_all(\"tr\"):\n",
    "                columns = instance.find_all(\"td\")\n",
    "                if (columns[1].text.strip() == \"✅\") and (\n",
    "                    columns[2].text.strip() == \"✅\"\n",
    "                ):\n",
    "                    instance_list.append(instance.find(\"a\")[\"href\"])\n",
    "            return instance_list\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _test_all_instances(self, endpoint, no_print=False):\n",
    "        \"\"\"\n",
    "        Test all Nitter instances when a high number of retries is detected\n",
    "\n",
    "        :param endpoint: endpoint to use\n",
    "        :param no_print: True if no output should be printed\n",
    "        \"\"\"\n",
    "        if not no_print:\n",
    "            print(\"High number of retries detected. Testing all instances...\")\n",
    "        working_instances = []\n",
    "\n",
    "        for instance in tqdm(self.instances, desc=\"Testing instances\"):\n",
    "            self._initialize_session(instance)\n",
    "            req_session = requests.Session()\n",
    "            req_session.headers.update(\n",
    "                {\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0\"\n",
    "                }\n",
    "            )\n",
    "            try:\n",
    "                r = req_session.get(\n",
    "                    instance + endpoint,\n",
    "                    cookies={\"hlsPlayback\": \"on\"},\n",
    "                    timeout=10,\n",
    "                )\n",
    "                if r.ok:\n",
    "                    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "                    if soup is not None and len(\n",
    "                        soup.find_all(\"div\", class_=\"timeline-item\")\n",
    "                    ):\n",
    "                        working_instances.append(instance)\n",
    "            except:\n",
    "                pass\n",
    "        if not no_print:\n",
    "            print(\"New working instances:\", \", \".join(working_instances))\n",
    "        self.working_instances = working_instances\n",
    "\n",
    "    def _get_new_instance(self, message):\n",
    "        instance = self.get_random_instance()\n",
    "        logging.warning(f\"{message}. Trying {instance}\")\n",
    "        return instance\n",
    "    \n",
    "    def _check_error_page(self, soup):\n",
    "        \"\"\"\n",
    "        Check if the page contains an error. If so, print the error and return None\n",
    "\n",
    "        :param soup: page to check\n",
    "        :return: None if error is found, soup otherwise\n",
    "        \"\"\"\n",
    "        if not soup.find(\n",
    "            lambda tag: tag.name == \"div\"\n",
    "            and (\n",
    "                tag.get(\"class\") == [\"timeline-item\"]\n",
    "                or tag.get(\"class\") == [\"timeline-item\", \"thread\"]\n",
    "            )\n",
    "        ):\n",
    "            if soup.find(\"div\", class_=\"error-panel\"):\n",
    "                message = (\n",
    "                    f\"Fetching error: \"\n",
    "                    + soup.find(\"div\", class_=\"error-panel\").find(\"span\").text.strip()\n",
    "                )\n",
    "            else:\n",
    "                if soup.find(\"div\", class_=\"timeline-header timeline-protected\"):\n",
    "                    message = \"Account is protected\"\n",
    "                else:\n",
    "                    message = f\"Empty page on {self.instance}\"\n",
    "            logging.warning(message)\n",
    "            soup = None\n",
    "        return soup\n",
    "\n",
    "    def _get_page(self, endpoint, max_retries=5):\n",
    "        \"\"\"\n",
    "        Download page from Nitter instance\n",
    "\n",
    "        :param endpoint: endpoint to use\n",
    "        :param max_retries: max number of retries, default 5\n",
    "        :return: page content, or None if max retries reached\n",
    "        \"\"\"\n",
    "        keep_trying = True\n",
    "        soup = None\n",
    "        while keep_trying and (self.retry_count < max_retries):\n",
    "            try:\n",
    "                r = self.r.get(\n",
    "                    self.instance + endpoint,\n",
    "                    cookies={\"hlsPlayback\": \"on\", \"infiniteScroll\": \"\"},\n",
    "                    timeout=10,\n",
    "                )\n",
    "            except:\n",
    "                if self.retry_count == max_retries // 2:\n",
    "                    if not self.skip_instance_check:\n",
    "                        self._test_all_instances(endpoint)\n",
    "                        if not self.working_instances:\n",
    "                            logging.warning(\n",
    "                                \"All instances are unreachable. Check your request and try again.\"\n",
    "                            )\n",
    "                            return None\n",
    "                if not self.skip_instance_check:\n",
    "                    self._initialize_session(\n",
    "                        instance=self._get_new_instance(f\"{self.instance} unreachable\")\n",
    "                    )\n",
    "                self.retry_count += 1\n",
    "                self.cooldown_count = 0\n",
    "                self.session_reset = True\n",
    "                sleep(1)\n",
    "                continue\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "            if r.ok:\n",
    "                self.session_reset = False\n",
    "                soup = self._check_error_page(soup)\n",
    "                keep_trying = False\n",
    "            else:\n",
    "                soup = self._check_error_page(soup)\n",
    "                if soup is None:\n",
    "                    keep_trying = False\n",
    "                else:\n",
    "                    if self.retry_count == max_retries // 2:\n",
    "                        if not self.skip_instance_check:\n",
    "                            self._test_all_instances(endpoint)\n",
    "                            if not self.working_instances:\n",
    "                                logging.warning(\n",
    "                                    \"All instances are unreachable. Check your request and try again.\"\n",
    "                                )\n",
    "                                soup = None\n",
    "                                keep_trying = False\n",
    "                        else:\n",
    "                            self.retry_count += 1\n",
    "                    else:\n",
    "                        if \"cursor\" in endpoint:\n",
    "                            if not self.session_reset:\n",
    "                                logging.warning(\n",
    "                                    \"Cooldown reached, trying again in 20 seconds\"\n",
    "                                )\n",
    "                                self.cooldown_count += 1\n",
    "                                sleep(20)\n",
    "                            if self.cooldown_count >= 5 and not self.session_reset:\n",
    "                                if not self.skip_instance_check:\n",
    "                                    self._initialize_session()\n",
    "                                else:\n",
    "                                    self._initialize_session(self.instance)\n",
    "                                self.session_reset = True\n",
    "                                self.cooldown_count = 0\n",
    "                            elif self.session_reset:\n",
    "                                if not self.skip_instance_check:\n",
    "                                    self._initialize_session(\n",
    "                                        self._get_new_instance(\n",
    "                                            f\"Error fetching {self.instance}\"\n",
    "                                        )\n",
    "                                    )\n",
    "                        else:\n",
    "                            self.cooldown_count = 0\n",
    "                            if not self.skip_instance_check:\n",
    "                                self._initialize_session(\n",
    "                                    self._get_new_instance(\n",
    "                                        f\"Error fetching {self.instance}\"\n",
    "                                    )\n",
    "                                )\n",
    "                        self.retry_count += 1\n",
    "            sleep(2)\n",
    "\n",
    "        if self.retry_count >= max_retries:\n",
    "            logging.warning(\"Max retries reached. Check your request and try again.\")\n",
    "            soup = None\n",
    "        self.retry_count = 0\n",
    "\n",
    "        return soup\n",
    "\n",
    "    def _get_quoted_media(self, quoted_tweet, is_encrypted):\n",
    "        \"\"\"\n",
    "        Extract media from a quoted tweet\n",
    "\n",
    "        :param quoted_tweet: tweet to extract media from\n",
    "        :param is_encrypted: True if instance uses encrypted media\n",
    "        :return: lists of images, videos and gifs, or empty lists if no media is found\n",
    "        \"\"\"\n",
    "        quoted_pictures, quoted_videos, quoted_gifs = [], [], []\n",
    "        if quoted_tweet.find(\"div\", class_=\"attachments\"):\n",
    "            if is_encrypted:\n",
    "                quoted_pictures = [\n",
    "                    \"https://pbs.twimg.com/\"\n",
    "                    + b64decode(img[\"src\"].split(\"/\")[-1].encode(\"utf-8\"))\n",
    "                    .decode(\"utf-8\")\n",
    "                    .split(\"?\")[0]\n",
    "                    for img in quoted_tweet.find(\"div\", class_=\"attachments\").find_all(\n",
    "                        \"img\"\n",
    "                    )\n",
    "                ]\n",
    "                quoted_videos = [\n",
    "                    b64decode(video[\"data-url\"].split(\"/\")[-1].encode(\"utf-8\")).decode(\n",
    "                        \"utf-8\"\n",
    "                    )\n",
    "                    if \"data-url\" in video.attrs\n",
    "                    else video.find(\"source\")[\"src\"]\n",
    "                    for video in quoted_tweet.find(\n",
    "                        \"div\", class_=\"attachments\"\n",
    "                    ).find_all(\"video\", class_=\"\")\n",
    "                ]\n",
    "                quoted_gifs = [\n",
    "                    \"https://\"\n",
    "                    + b64decode(\n",
    "                        gif.source[\"src\"].split(\"/\")[-1].encode(\"utf-8\")\n",
    "                    ).decode(\"utf-8\")\n",
    "                    for gif in quoted_tweet.find(\"div\", class_=\"attachments\").find_all(\n",
    "                        \"video\", class_=\"gif\"\n",
    "                    )\n",
    "                ]\n",
    "            else:\n",
    "                quoted_pictures = [\n",
    "                    \"https://pbs.twimg.com\"\n",
    "                    + unquote(img[\"src\"].split(\"/pic\")[1]).split(\"?\")[0]\n",
    "                    for img in quoted_tweet.find(\"div\", class_=\"attachments\").find_all(\n",
    "                        \"img\"\n",
    "                    )\n",
    "                ]\n",
    "                quoted_videos = [\n",
    "                    unquote(\"https\" + video[\"data-url\"].split(\"https\")[1])\n",
    "                    if \"data-url\" in video.attrs\n",
    "                    else unquote(video.find(\"source\")[\"src\"])\n",
    "                    for video in quoted_tweet.find(\n",
    "                        \"div\", class_=\"attachments\"\n",
    "                    ).find_all(\"video\", class_=\"\")\n",
    "                ]\n",
    "                quoted_gifs = [\n",
    "                    unquote(\"https://\" + gif.source[\"src\"].split(\"/pic/\")[1])\n",
    "                    for gif in quoted_tweet.find(\"div\", class_=\"attachments\").find_all(\n",
    "                        \"video\", class_=\"gif\"\n",
    "                    )\n",
    "                ]\n",
    "        return quoted_pictures, quoted_videos, quoted_gifs\n",
    "\n",
    "    def _get_tweet_media(self, tweet, is_encrypted):\n",
    "        \"\"\"\n",
    "        Extract media from a tweet\n",
    "\n",
    "        :param tweet: tweet to extract media from\n",
    "        :param is_encrypted: True if instance uses encrypted media\n",
    "        :return: lists of images, videos and gifs, or empty lists if no media is found\n",
    "        \"\"\"\n",
    "        pictures, videos, gifs = [], [], []\n",
    "        if tweet.find(\"div\", class_=\"tweet-body\").find(\n",
    "            \"div\", class_=\"attachments\", recursive=False\n",
    "        ):\n",
    "            if is_encrypted:\n",
    "                pictures = [\n",
    "                    \"https://pbs.twimg.com/\"\n",
    "                    + b64decode(img[\"src\"].split(\"/\")[-1].encode(\"utf-8\"))\n",
    "                    .decode(\"utf-8\")\n",
    "                    .split(\"?\")[0]\n",
    "                    for img in tweet.find(\"div\", class_=\"tweet-body\")\n",
    "                    .find(\"div\", class_=\"attachments\", recursive=False)\n",
    "                    .find_all(\"img\")\n",
    "                ]\n",
    "                videos = [\n",
    "                    b64decode(video[\"data-url\"].split(\"/\")[-1].encode(\"utf-8\")).decode(\n",
    "                        \"utf-8\"\n",
    "                    )\n",
    "                    if \"data-url\" in video.attrs\n",
    "                    else video.find(\"source\")[\"src\"]\n",
    "                    for video in tweet.find(\"div\", class_=\"tweet-body\")\n",
    "                    .find(\"div\", class_=\"attachments\", recursive=False)\n",
    "                    .find_all(\"video\", class_=\"\")\n",
    "                ]\n",
    "                gifs = [\n",
    "                    \"https://\"\n",
    "                    + b64decode(\n",
    "                        gif.source[\"src\"].split(\"/\")[-1].encode(\"utf-8\")\n",
    "                    ).decode(\"utf-8\")\n",
    "                    for gif in tweet.find(\"div\", class_=\"tweet-body\")\n",
    "                    .find(\"div\", class_=\"attachments\", recursive=False)\n",
    "                    .find_all(\"video\", class_=\"gif\")\n",
    "                ]\n",
    "            else:\n",
    "                pictures = [\n",
    "                    \"https://pbs.twimg.com\"\n",
    "                    + unquote(img[\"src\"].split(\"/pic\")[1]).split(\"?\")[0]\n",
    "                    for img in tweet.find(\"div\", class_=\"tweet-body\")\n",
    "                    .find(\"div\", class_=\"attachments\", recursive=False)\n",
    "                    .find_all(\"img\")\n",
    "                ]\n",
    "                videos = [\n",
    "                    unquote(\"https\" + video[\"data-url\"].split(\"https\")[1])\n",
    "                    if \"data-url\" in video.attrs\n",
    "                    else video.find(\"source\")[\"src\"]\n",
    "                    for video in tweet.find(\"div\", class_=\"tweet-body\")\n",
    "                    .find(\"div\", class_=\"attachments\", recursive=False)\n",
    "                    .find_all(\"video\", class_=\"\")\n",
    "                ]\n",
    "                gifs = [\n",
    "                    unquote(\"https://\" + gif.source[\"src\"].split(\"/pic/\")[1])\n",
    "                    for gif in tweet.find(\"div\", class_=\"tweet-body\")\n",
    "                    .find(\"div\", class_=\"attachments\", recursive=False)\n",
    "                    .find_all(\"video\", class_=\"gif\")\n",
    "                ]\n",
    "        return pictures, videos, gifs\n",
    "\n",
    "    def _get_tweet_stats(self, tweet):\n",
    "        \"\"\"\n",
    "        Extract stats from a tweet\n",
    "\n",
    "        :param tweet: tweet to extract stats from\n",
    "        :return: dictionary of stats. If a stat is not found, it is set to 0\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"comments\": int(\n",
    "                tweet.find_all(\"span\", class_=\"tweet-stat\")[0]\n",
    "                .find(\"div\")\n",
    "                .text.strip()\n",
    "                .replace(\",\", \"\")\n",
    "                or 0\n",
    "            ),\n",
    "            \"retweets\": int(\n",
    "                tweet.find_all(\"span\", class_=\"tweet-stat\")[1]\n",
    "                .find(\"div\")\n",
    "                .text.strip()\n",
    "                .replace(\",\", \"\")\n",
    "                or 0\n",
    "            ),\n",
    "            \"quotes\": int(\n",
    "                tweet.find_all(\"span\", class_=\"tweet-stat\")[2]\n",
    "                .find(\"div\")\n",
    "                .text.strip()\n",
    "                .replace(\",\", \"\")\n",
    "                or 0\n",
    "            ),\n",
    "            \"likes\": int(\n",
    "                tweet.find_all(\"span\", class_=\"tweet-stat\")[3]\n",
    "                .find(\"div\")\n",
    "                .text.strip()\n",
    "                .replace(\",\", \"\")\n",
    "                or 0\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def _get_user(self, tweet, is_encrypted):\n",
    "        \"\"\"\n",
    "        Extract user from a tweet\n",
    "\n",
    "        :param tweet: tweet to extract user from\n",
    "        :param is_encrypted: True if instance uses encrypted media\n",
    "        :return: dictionary of user\n",
    "        \"\"\"\n",
    "        avatar = \"\"\n",
    "        profile_id = \"\"\n",
    "        if is_encrypted:\n",
    "            try:\n",
    "                avatar = \"https://pbs.twimg.com/\" + b64decode(\n",
    "                    tweet.find(\"img\", class_=\"avatar\")[\"src\"]\n",
    "                    .split(\"/\")[-1]\n",
    "                    .encode(\"utf-8\")\n",
    "                ).decode(\"utf-8\")\n",
    "            except:\n",
    "                avatar = \"\"\n",
    "\n",
    "            if tweet.find(\"img\", class_=\"avatar\"):\n",
    "                profile_id = (\n",
    "                    b64decode(\n",
    "                        tweet.find(\"img\", class_=\"avatar\")[\"src\"]\n",
    "                        .split(\"/enc/\")[1]\n",
    "                        .encode(\"utf-8\")\n",
    "                    )\n",
    "                    .decode(\"utf-8\")\n",
    "                    .split(\"/profile_images/\")[1]\n",
    "                    .split(\"/\")[0]\n",
    "                )\n",
    "        else:\n",
    "            avatar = \"https://pbs.twimg.com\" + unquote(\n",
    "                tweet.find(\"img\", class_=\"avatar\")[\"src\"].split(\"/pic\")[1]\n",
    "            )\n",
    "\n",
    "            if tweet.find(\"img\", class_=\"avatar\"):\n",
    "                profile_id = (\n",
    "                    unquote(tweet.find(\"img\", class_=\"avatar\")[\"src\"])\n",
    "                    .split(\"profile_images/\")[1]\n",
    "                    .split(\"/\")[0]\n",
    "                )\n",
    "        return {\n",
    "            \"name\": tweet.find(\"a\", class_=\"fullname\").text.strip(),\n",
    "            \"username\": tweet.find(\"a\", class_=\"username\").text.strip(),\n",
    "            \"profile_id\": profile_id,\n",
    "            \"avatar\": avatar,\n",
    "        }\n",
    "\n",
    "    def _get_tweet_date(self, tweet):\n",
    "        \"\"\"\n",
    "        Extract date from a tweet\n",
    "\n",
    "        :param tweet: tweet to extract date from\n",
    "        :return: date of tweet\n",
    "        \"\"\"\n",
    "        return (\n",
    "            tweet.find(\"span\", class_=\"tweet-date\")\n",
    "            .find(\"a\")[\"title\"]\n",
    "            .split(\"/\")[-1]\n",
    "            .split(\"#\")[0]\n",
    "            if tweet.find(\"span\", class_=\"tweet-date\")\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "    def _get_tweet_text(self, tweet):\n",
    "        \"\"\"\n",
    "        Extract text from a tweet\n",
    "\n",
    "        :param tweet: tweet to extract text from\n",
    "        :return: text of tweet\n",
    "        \"\"\"\n",
    "        return (\n",
    "            tweet.find(\"div\", class_=\"tweet-content media-body\")\n",
    "            .text.strip()\n",
    "            .replace(\"\\n\", \" \")\n",
    "            if tweet.find(\"div\", class_=\"tweet-content media-body\")\n",
    "            else tweet.find(\"div\", class_=\"quote-text\").text.strip().replace(\"\\n\", \" \")\n",
    "            if tweet.find(\"div\", class_=\"quote-text\")\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "    def _get_tweet_link(self, tweet):\n",
    "        \"\"\"\n",
    "        Extract link from a tweet\n",
    "\n",
    "        :param tweet: tweet to extract link from\n",
    "        :return: link of tweet\n",
    "        \"\"\"\n",
    "        return (\n",
    "            \"https://twitter.com\" + tweet.find(\"a\")[\"href\"] if tweet.find(\"a\") else \"\"\n",
    "        )\n",
    "\n",
    "    def _get_tweet_replies(self, tweet):\n",
    "        \"\"\"\n",
    "        Extract @-name from a tweet\n",
    "\n",
    "        :param tweet: tweet to extract @ from\n",
    "        :return: @name of tweet reply header\n",
    "        \"\"\"\n",
    "        if tweet.find(\"div\", class_=\"replying-to\"):\n",
    "            try:\n",
    "                output = []\n",
    "                for p in tweet.find(\"div\", class_=\"replying-to\").find_all(\"a\"):\n",
    "                    output.append(p.text)                \n",
    "            except:\n",
    "                output = tweet.find(\"div\", class_=\"replying-to\").find(\"a\").text\n",
    "        else: output = \"\"\n",
    "        return (output)\n",
    "\n",
    "    def _get_external_link(self, tweet):\n",
    "        \"\"\"\n",
    "        Extract external link from a tweet\n",
    "\n",
    "        :param tweet: tweet to extract external link from\n",
    "        :return: external link of tweet\n",
    "        \"\"\"\n",
    "        return (\n",
    "            tweet.find(\"a\", class_=\"card-container\")[\"href\"]\n",
    "            if tweet.find(\"a\", class_=\"card-container\")\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "    def _extract_tweet(self, tweet, is_encrypted):\n",
    "        \"\"\"\n",
    "        Extract content from a tweet\n",
    "\n",
    "        :param tweet: tweet to extract content from\n",
    "        :param is_encrypted: True if instance uses encrypted media\n",
    "        :return: dictionary of content for the tweet\n",
    "        \"\"\"\n",
    "        # Replace link text with link\n",
    "        if tweet.find_all(\"a\"):\n",
    "            for link in tweet.find_all(\"a\"):\n",
    "                if \"https\" in link[\"href\"]:\n",
    "                    link.replace_with(link[\"href\"])\n",
    "\n",
    "        # Extract the quoted tweet\n",
    "        quoted_tweet = (\n",
    "            tweet.find(\"div\", class_=\"quote\")\n",
    "            if tweet.find(\"div\", class_=\"quote\")\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        # Extract media from the quoted tweet\n",
    "        if quoted_tweet:\n",
    "            deleted = False\n",
    "            if quoted_tweet[\"class\"] == [\"quote\", \"unavailable\"]:\n",
    "                deleted = True\n",
    "            (\n",
    "                quoted_pictures,\n",
    "                quoted_videos,\n",
    "                quoted_gifs,\n",
    "            ) = self._get_quoted_media(quoted_tweet, is_encrypted)\n",
    "\n",
    "        # is reply\n",
    "\t\n",
    "\t# Extract media from the tweet\n",
    "        pictures, videos, gifs = self._get_tweet_media(tweet, is_encrypted)\n",
    "\n",
    "        return {\n",
    "            \"link\": self._get_tweet_link(tweet),\n",
    "            \"text\": self._get_tweet_text(tweet),\n",
    "            \"user\": self._get_user(tweet, is_encrypted),\n",
    "            \"date\": self._get_tweet_date(tweet),\n",
    "            \"is-retweet\": tweet.find(\"div\", class_=\"retweet-header\") is not None,\n",
    "            \"is-reply\": self._get_tweet_replies(tweet),\n",
    "#            \"is-reply\": tweet.find(\"div\", class=\"replying-to\") is not None,\n",
    "            \"external-link\": self._get_external_link(tweet),\n",
    "            \"quoted-post\": {\n",
    "                \"link\": self._get_tweet_link(quoted_tweet) if not deleted else \"\",\n",
    "                \"text\": self._get_tweet_text(quoted_tweet) if not deleted else \"\",\n",
    "                \"user\": self._get_user(quoted_tweet, is_encrypted)\n",
    "                if not deleted\n",
    "                else {},\n",
    "                \"date\": self._get_tweet_date(quoted_tweet) if not deleted else \"\",\n",
    "                \"pictures\": quoted_pictures,\n",
    "                \"videos\": quoted_videos,\n",
    "                \"gifs\": quoted_gifs,\n",
    "            }\n",
    "            if quoted_tweet\n",
    "            else {},\n",
    "            \"stats\": self._get_tweet_stats(tweet),\n",
    "            \"pictures\": pictures,\n",
    "            \"videos\": videos,\n",
    "            \"gifs\": gifs,\n",
    "        }\n",
    "\n",
    "    def _check_date_validity(self, date):\n",
    "        \"\"\"\n",
    "        Check if a date is valid\n",
    "\n",
    "        :param date: date to check\n",
    "        :return: True if date is valid\n",
    "        \"\"\"\n",
    "        to_return = True\n",
    "        if not match(r\"^\\d{4}-\\d{2}-\\d{2}$\", date):\n",
    "            to_return = False\n",
    "        try:\n",
    "            year, month, day = [int(number) for number in date.split(\"-\")]\n",
    "            datetime(year=year, month=month, day=day)\n",
    "        except:\n",
    "            to_return = False\n",
    "\n",
    "        if not (\n",
    "            datetime(year=2006, month=3, day=21)\n",
    "            < datetime(year=year, month=month, day=day)\n",
    "            <= datetime.now()\n",
    "        ):\n",
    "            to_return = False\n",
    "\n",
    "        return to_return\n",
    "\n",
    "    def _search(\n",
    "        self,\n",
    "        term,\n",
    "        mode,\n",
    "        number,\n",
    "        since,\n",
    "        until,\n",
    "        near,\n",
    "        language,\n",
    "        to,\n",
    "        filters,\n",
    "        exclude,\n",
    "        max_retries,\n",
    "        instance,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Scrape the specified search terms from Nitter\n",
    "\n",
    "        :param term: term to seach for\n",
    "        :param mode: search mode.\n",
    "        :param number: number of tweets to scrape.\n",
    "        :param since: date to start scraping from.\n",
    "        :param until: date to stop scraping at.\n",
    "        :param near: location to search near.\n",
    "        :param language: language of the tweets.\n",
    "        :param to: user to which the tweets are directed.\n",
    "        :param filters: list of filters to apply.\n",
    "        :param exclude: list of filters to exclude.\n",
    "        :param max_retries: max retries to scrape a page.\n",
    "        :param instance: Nitter instance to use.\n",
    "        :return: dictionary of tweets and threads for the term.\n",
    "        \"\"\"\n",
    "        tweets = {\"tweets\": [], \"threads\": []}\n",
    "        if mode == \"hashtag\":\n",
    "            endpoint = \"/search?f=tweets&q=%23\" + term\n",
    "        elif mode == \"term\":\n",
    "            endpoint = \"/search?f=tweets&q=\" + term\n",
    "        elif mode == \"user\":\n",
    "            if since or until:\n",
    "                endpoint = f\"/{term}/search?f=tweets&q=\"\n",
    "            else:\n",
    "                endpoint = f\"/{term}\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Use 'term', 'hashtag', or 'user'.\")\n",
    "\n",
    "        self._initialize_session(instance)\n",
    "\n",
    "        if language:\n",
    "            endpoint += f\"+lang%3A{language}\"\n",
    "\n",
    "        if to:\n",
    "            endpoint += f\"+to%3A{to}\"\n",
    "\n",
    "        if since:\n",
    "            if self._check_date_validity(since):\n",
    "                endpoint += f\"&since={since}\"\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid 'since' date. Use the YYYY-MM-DD format and make sure the date is valid.\"\n",
    "                )\n",
    "\n",
    "        if until:\n",
    "            if self._check_date_validity(until):\n",
    "                endpoint += f\"&until={until}\"\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid 'until' date. Use the YYYY-MM-DD format and make sure the date is valid.\"\n",
    "                )\n",
    "\n",
    "        if near:\n",
    "            endpoint += f\"&near={near}\"\n",
    "\n",
    "        if filters:\n",
    "            for f in filters:\n",
    "                if f not in valid_filters:\n",
    "                    raise ValueError(\n",
    "                        f\"Invalid filter '{f}'. Valid filters are: {', '.join(valid_filters)}\"\n",
    "                    )\n",
    "                endpoint += f\"&f-{f}=on\"\n",
    "\n",
    "        if exclude:\n",
    "            for e in exclude:\n",
    "                if e not in valid_filters:\n",
    "                    raise ValueError(\n",
    "                        f\"Invalid exclusion filter '{e}'. Valid filters are: {', '.join(valid_filters)}\"\n",
    "                    )\n",
    "                endpoint += f\"&e-{e}=on\"\n",
    "\n",
    "        if mode != \"user\":\n",
    "            if \"?\" in endpoint:\n",
    "                endpoint += \"&scroll=false\"\n",
    "            else:\n",
    "                endpoint += \"?scroll=false\"\n",
    "\n",
    "        soup = self._get_page(endpoint, max_retries)\n",
    "\n",
    "        if soup is None:\n",
    "            return tweets\n",
    "\n",
    "        is_encrypted = self._is_instance_encrypted()\n",
    "\n",
    "        already_scraped = set()\n",
    "\n",
    "        number = float(\"inf\") if number == -1 else number\n",
    "        keep_scraping = True\n",
    "        while keep_scraping:\n",
    "            thread = []\n",
    "\n",
    "            for tweet in soup.find_all(\"div\", class_=\"timeline-item\"):\n",
    "                if len(tweet[\"class\"]) == 1:\n",
    "                    to_append = self._extract_tweet(tweet, is_encrypted)\n",
    "                    # Extract tweets\n",
    "                    if len(tweets[\"tweets\"]) + len(tweets[\"threads\"]) < number:\n",
    "                        if self._get_tweet_link(tweet) not in already_scraped:\n",
    "                            tweets[\"tweets\"].append(to_append)\n",
    "                            already_scraped.add(self._get_tweet_link(tweet))\n",
    "                    else:\n",
    "                        keep_scraping = False\n",
    "                        break\n",
    "                else:\n",
    "                    if \"thread\" in tweet[\"class\"]:\n",
    "                        to_append = self._extract_tweet(tweet, is_encrypted)\n",
    "                        # Extract threads\n",
    "                        if self._get_tweet_link(tweet) not in already_scraped:\n",
    "                            thread.append(to_append)\n",
    "                            already_scraped.add(self._get_tweet_link(tweet))\n",
    "\n",
    "                        if len(tweet[\"class\"]) == 3:\n",
    "                            tweets[\"threads\"].append(thread)\n",
    "                            thread = []\n",
    "\n",
    "            logging.info(\n",
    "                f\"Current stats for {term}: {len(tweets['tweets'])} tweets, {len(tweets['threads'])} threads...\"\n",
    "            )\n",
    "            if (\n",
    "                not (since and until)\n",
    "                and not (since)\n",
    "                and len(tweets[\"tweets\"]) + len(tweets[\"threads\"]) >= number\n",
    "            ):\n",
    "                keep_scraping = False\n",
    "            else:\n",
    "                sleep(uniform(1, 2))\n",
    "\n",
    "                # Go to the next page\n",
    "                show_more_buttons = soup.find_all(\"div\", class_=\"show-more\")\n",
    "                if soup.find_all(\"div\", class_=\"show-more\"):\n",
    "                    if mode == \"user\":\n",
    "                        if since or until:\n",
    "                            next_page = (\n",
    "                                f\"/{term}/search?\"\n",
    "                                + show_more_buttons[-1].find(\"a\")[\"href\"].split(\"?\")[-1]\n",
    "                            )\n",
    "                        else:\n",
    "                            next_page = (\n",
    "                                f\"/{term}?\"\n",
    "                                + show_more_buttons[-1].find(\"a\")[\"href\"].split(\"?\")[-1]\n",
    "                            )\n",
    "                    else:\n",
    "                        next_page = \"/search\" + show_more_buttons[-1].find(\"a\")[\"href\"]\n",
    "                    soup = self._get_page(next_page, max_retries)\n",
    "                    if soup is None:\n",
    "                        keep_scraping = False\n",
    "                else:\n",
    "                    keep_scraping = False\n",
    "        return tweets\n",
    "\n",
    "    def _search_dispatch(self, args):\n",
    "        return self._search(*args)\n",
    "\n",
    "    def get_random_instance(self):\n",
    "        \"\"\"\n",
    "        Get a random Nitter instance\n",
    "\n",
    "        :return: URL of random Nitter instance\n",
    "        \"\"\"\n",
    "        return random.choice(self.working_instances)\n",
    "\n",
    "    def get_tweets(\n",
    "        self,\n",
    "        terms,\n",
    "        mode=\"term\",\n",
    "        number=-1,\n",
    "        since=None,\n",
    "        until=None,\n",
    "        near=None,\n",
    "        language=None,\n",
    "        to=None,\n",
    "        filters=None,\n",
    "        exclude=None,\n",
    "        max_retries=5,\n",
    "        instance=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Scrape the specified term from Nitter\n",
    "\n",
    "        :param terms: string/s to search for\n",
    "        :param mode: search mode. Default is 'term', can also be 'hashtag' or 'user'\n",
    "        :param number: number of tweets to scrape. Default is -1 (to not set a limit).\n",
    "        :param since: date to start scraping from, formatted as YYYY-MM-DD. Default is None\n",
    "        :param until: date to stop scraping at, formatted as YYYY-MM-DD. Default is None\n",
    "        :param near: near location of the tweets. Default is None (anywhere)\n",
    "        :param language: language of the tweets. Default is None (any language)\n",
    "        :param to: user to which the tweets are directed. Default is None (any user)\n",
    "        :param filters: list of filters to apply. Default is None\n",
    "        :param exclude: list of filters to exclude. Default is None\n",
    "        :param max_retries: max retries to scrape a page. Default is 5\n",
    "        :param instance: Nitter instance to use. Default is None\n",
    "        :return: dictionary or array with dictionaries (in case of multiple terms) of the tweets and threads for the provided terms\n",
    "        \"\"\"\n",
    "        if type(terms) == str:\n",
    "            term = terms.strip()\n",
    "\n",
    "            return self._search(\n",
    "                term,\n",
    "                mode,\n",
    "                number,\n",
    "                since,\n",
    "                until,\n",
    "                near,\n",
    "                language,\n",
    "                to,\n",
    "                filters,\n",
    "                exclude,\n",
    "                max_retries,\n",
    "                instance,\n",
    "            )\n",
    "        elif len(terms) == 1:\n",
    "            term = terms[0].strip()\n",
    "\n",
    "            return self._search(\n",
    "                term,\n",
    "                mode,\n",
    "                number,\n",
    "                since,\n",
    "                until,\n",
    "                near,\n",
    "                language,\n",
    "                to,\n",
    "                filters,\n",
    "                exclude,\n",
    "                max_retries,\n",
    "                instance,\n",
    "            )\n",
    "        else:\n",
    "            if len(terms) > cpu_count():\n",
    "                raise ValueError(\n",
    "                    f\"Too many terms. Maximum number of terms is {cpu_count()}\"\n",
    "                )\n",
    "\n",
    "            args = [\n",
    "                (\n",
    "                    term.strip(),\n",
    "                    mode,\n",
    "                    number,\n",
    "                    since,\n",
    "                    until,\n",
    "                    near,\n",
    "                    language,\n",
    "                    to,\n",
    "                    filters,\n",
    "                    exclude,\n",
    "                    max_retries,\n",
    "                    instance,\n",
    "                )\n",
    "                for term in terms\n",
    "            ]\n",
    "            with Pool(len(terms)) as p:\n",
    "                results = list(p.map(self._search_dispatch, args))\n",
    "\n",
    "            return results\n",
    "\n",
    "    def get_profile_info(self, username, max_retries=5, instance=None):\n",
    "        \"\"\"\n",
    "        Get profile information for a user\n",
    "\n",
    "        :param username: username of the page to scrape\n",
    "        :param max_retries: max retries to scrape a page. Default is 5\n",
    "        :param instance: Nitter instance to use. Default is None\n",
    "        :return: dictionary of the profile's information\n",
    "        \"\"\"\n",
    "        self._initialize_session(instance)\n",
    "        username = sub(r\"[^A-Za-z0-9_+-:]\", \"\", username)\n",
    "        soup = self._get_page(f\"/{username}\", max_retries)\n",
    "        if soup is None:\n",
    "            return None\n",
    "\n",
    "        is_encrypted = self._is_instance_encrypted()\n",
    "        # Extract id if the banner exists, no matter if the instance uses base64 or not\n",
    "        if soup.find(\"div\", class_=\"profile-banner\").find(\"img\") and is_encrypted:\n",
    "            profile_id = (\n",
    "                b64decode(\n",
    "                    soup.find(\"div\", class_=\"profile-banner\")\n",
    "                    .find(\"img\")[\"src\"]\n",
    "                    .split(\"/enc/\")[1]\n",
    "                    .encode(\"utf-8\")\n",
    "                )\n",
    "                .decode(\"utf-8\")\n",
    "                .split(\"/profile_banners/\")[1]\n",
    "                .split(\"/\")[0]\n",
    "            )\n",
    "        elif soup.find(\"div\", class_=\"profile-banner\").find(\"img\"):\n",
    "            profile_id = (\n",
    "                unquote(soup.find(\"div\", class_=\"profile-banner\").find(\"img\")[\"src\"])\n",
    "                .split(\"profile_banners/\")[1]\n",
    "                .split(\"/\")[0]\n",
    "            )\n",
    "        else:\n",
    "            profile_id = \"\"\n",
    "\n",
    "        # Extract profile image, no matter if the instance uses base64 or not\n",
    "        if soup.find(\"a\", class_=\"profile-card-avatar\").find(\"img\") and is_encrypted:\n",
    "            profile_image = \"https://\" + b64decode(\n",
    "                soup.find(\"a\", class_=\"profile-card-avatar\")\n",
    "                .find(\"img\")[\"src\"]\n",
    "                .split(\"/enc/\")[1]\n",
    "                .encode(\"utf-8\")\n",
    "            ).decode(\"utf-8\")\n",
    "        elif soup.find(\"a\", class_=\"profile-card-avatar\").find(\"img\"):\n",
    "            profile_image = (\n",
    "                \"https://\"\n",
    "                + unquote(\n",
    "                    soup.find(\"a\", class_=\"profile-card-avatar\").find(\"img\")[\"src\"]\n",
    "                ).split(\"/pic/\")[1]\n",
    "            )\n",
    "        else:\n",
    "            profile_image = \"\"\n",
    "\n",
    "        icon_container = (\n",
    "            soup.find(\"div\", class_=\"photo-rail-header\").find(\n",
    "                \"div\", class_=\"icon-container\"\n",
    "            )\n",
    "            if soup.find(\"div\", class_=\"photo-rail-header\")\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"image\": profile_image,\n",
    "            \"name\": soup.find(\"a\", class_=\"profile-card-fullname\").text.strip(),\n",
    "            \"username\": soup.find(\"a\", class_=\"profile-card-username\").text.strip(),\n",
    "            \"id\": profile_id,\n",
    "            \"bio\": soup.find(\"div\", class_=\"profile-bio\").p.text.strip()\n",
    "            if soup.find(\"div\", class_=\"profile-bio\")\n",
    "            else \"\",\n",
    "            \"location\": soup.find(\"div\", class_=\"profile-location\")\n",
    "            .find_all(\"span\")[-1]\n",
    "            .text.strip()\n",
    "            if soup.find(\"div\", class_=\"profile-location\")\n",
    "            else \"\",\n",
    "            \"website\": soup.find(\"div\", class_=\"profile-website\").find(\"a\")[\"href\"]\n",
    "            if soup.find(\"div\", class_=\"profile-website\")\n",
    "            else \"\",\n",
    "            \"joined\": soup.find(\"div\", class_=\"profile-joindate\").find(\"span\")[\"title\"],\n",
    "            \"stats\": {\n",
    "                \"tweets\": int(\n",
    "                    soup.find(\"ul\", class_=\"profile-statlist\")\n",
    "                    .find(\"li\", class_=\"posts\")\n",
    "                    .find_all(\"span\")[1]\n",
    "                    .text.strip()\n",
    "                    .replace(\",\", \"\")\n",
    "                ),\n",
    "                \"following\": int(\n",
    "                    soup.find(\"ul\", class_=\"profile-statlist\")\n",
    "                    .find(\"li\", class_=\"following\")\n",
    "                    .find_all(\"span\")[1]\n",
    "                    .text.strip()\n",
    "                    .replace(\",\", \"\")\n",
    "                ),\n",
    "                \"followers\": int(\n",
    "                    soup.find(\"ul\", class_=\"profile-statlist\")\n",
    "                    .find(\"li\", class_=\"followers\")\n",
    "                    .find_all(\"span\")[1]\n",
    "                    .text.strip()\n",
    "                    .replace(\",\", \"\")\n",
    "                ),\n",
    "                \"likes\": int(\n",
    "                    soup.find(\"ul\", class_=\"profile-statlist\")\n",
    "                    .find(\"li\", class_=\"likes\")\n",
    "                    .find_all(\"span\")[1]\n",
    "                    .text.strip()\n",
    "                    .replace(\",\", \"\")\n",
    "                ),\n",
    "                \"media\": int(\n",
    "                    icon_container.text.strip().replace(\",\", \"\").split(\" \")[0]\n",
    "                    if icon_container\n",
    "                    else 0\n",
    "                ),\n",
    "            },\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd47f5-46c2-4053-a4df-89f023787890",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = Nitter(log_level=1, skip_instance_check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac4856-d064-4540-9247-49ad2c8e1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac97f3-85cc-44e0-9e9e-82e66715ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MetzTilly = scraper.get_tweets(\"MetzTilly\", mode='user', since='2020-01-01')\n",
    "\n",
    "MetzTilly_tweets = []\n",
    "\n",
    "for tweet in MetzTilly['tweets']:\n",
    "    data = [tweet['link'], tweet['date'], tweet['stats']['retweets'], tweet['stats']['likes'], tweet['stats']['comments'], tweet['stats']['quotes'], tweet['text'], tweet['is-reply'], tweet['is-retweet']]\n",
    "    MetzTilly_tweets.append(data)\n",
    "\n",
    "data8 = pd.DataFrame(MetzTilly_tweets, columns = ['link', 'date', 'retweets', 'likes', 'comments', 'quotes', 'text', 'replied-to', 'retweet'])\n",
    "data8.to_csv('MetzTilly_tweets.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cabee5-d3b3-4b72-a0d7-876dcea117f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07deb8c-0078-415b-bf27-180a82bafdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = data8['replied-to'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2398595-a9f4-4e95-b0f4-9bfd6d1b2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "AWIntergroup = scraper.get_tweets(\"AWIntergroup\", mode='user', since='2020-01-01')\n",
    "\n",
    "AWIntergroup_tweets = []\n",
    "\n",
    "for tweet in AWIntergroup['tweets']:\n",
    "    data = [tweet['link'], tweet['date'], tweet['stats']['retweets'], tweet['stats']['likes'], tweet['stats']['comments'], tweet['stats']['quotes'], tweet['text'], tweet['is-reply'], tweet['is-retweet']]\n",
    "    AWIntergroup_tweets.append(data)\n",
    "\n",
    "data9 = pd.DataFrame(AWIntergroup_tweets, columns = ['link', 'date', 'retweets', 'likes', 'comments', 'quotes', 'text', 'replied-to', 'retweet'])\n",
    "data9.to_csv('AWIntergroup_tweets.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36e8a6-0684-4c80-b507-d6a86df68a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61d4c5-294e-4df9-aa3c-7d611b09cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Act4AnimalsEU = scraper.get_tweets(\"Act4AnimalsEU\", mode='user', since='2020-01-01')\n",
    "\n",
    "Act4AnimalsEU_tweets = []\n",
    "\n",
    "for tweet in Act4AnimalsEU['tweets']:\n",
    "    data = [tweet['link'], tweet['date'], tweet['stats']['retweets'], tweet['stats']['likes'], tweet['stats']['comments'], tweet['stats']['quotes'], tweet['text'], tweet['\"quoted-post\"']['user'], tweet['is-reply'], tweet['is-retweet']]\n",
    "    Act4AnimalsEU_tweets.append(data)\n",
    "\n",
    "data10 = pd.DataFrame(Act4AnimalsEU_tweets, columns = ['link', 'date', 'retweets', 'likes', 'comments', 'quotes', 'text', 'quoted', 'replied-to', 'retweet'])\n",
    "data10.to_csv('Act4AnimalsEU_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a842f-6a33-4d73-8aa0-24da6819f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2dcefb-bad1-4297-9d6e-eafd269b765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "LadyFreethinker = scraper.get_tweets(\"LadyFreethinker\", mode='user', since='2020-01-01')\n",
    "\n",
    "LadyFreethinker_tweets = []\n",
    "\n",
    "for tweet in LadyFreethinker['tweets']:\n",
    "    data = [tweet['link'], tweet['date'], tweet['stats']['retweets'], tweet['stats']['likes'], tweet['stats']['comments'], tweet['stats']['quotes'], tweet['text'], tweet['is-reply'], tweet['is-retweet']]\n",
    "    LadyFreethinker_tweets.append(data)\n",
    "\n",
    "data11 = pd.DataFrame(LadyFreethinker_tweets, columns = ['link', 'date', 'retweets', 'likes', 'comments', 'quotes', 'text', 'replied-to', 'retweet'])\n",
    "data11.to_csv('LadyFreethinker_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf2e23-cda2-4295-998f-7be49842d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6dbafe-9930-409f-b108-c67abde3cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ProtectWldlife = scraper.get_tweets(\"ProtectWldlife\", mode='user', since='2020-01-01')\n",
    "\n",
    "ProtectWldlife_tweets = []\n",
    "\n",
    "for tweet in ProtectWldlife['tweets']:\n",
    "    data = [tweet['link'], tweet['date'], tweet['stats']['retweets'], tweet['stats']['likes'], tweet['stats']['comments'], tweet['stats']['quotes'], tweet['text'], tweet['is-reply'], tweet['is-retweet']]\n",
    "    ProtectWldlife_tweets.append(data)\n",
    "\n",
    "data12 = pd.DataFrame(ProtectWldlife_tweets, columns = ['link', 'date', 'retweets', 'likes', 'comments', 'quotes', 'text', 'replied-to', 'retweet'])\n",
    "data12.to_csv('ProtectWldlife_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5524c89f-f487-413a-987f-5bb4ba6b4168",
   "metadata": {},
   "outputs": [],
   "source": [
    "data12.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58d871e-29ca-4af3-8083-9686ae3bc91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data8 = pd.read_csv('MetzTilly_tweets.csv')\n",
    "data9 = pd.read_csv('AWIntergroup_tweets.csv')\n",
    "data10 = pd.read_csv('Act4AnimalsEU_tweets.csv')\n",
    "data11 = pd.read_csv('LadyFreethinker_tweets.csv')\n",
    "data12 = pd.read_csv('ProtectWldlife_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b4ca8-4a20-4f1c-9bde-e3570240782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji\n",
    "\n",
    "data8['text'] = data8.text.astype(str).apply(lambda x: demoji.replace(x,' '))\n",
    "data9['text'] = data9.text.astype(str).apply(lambda x: demoji.replace(x,' '))\n",
    "data10['text'] = data10.text.astype(str).apply(lambda x: demoji.replace(x,' '))\n",
    "data11['text'] = data11.text.astype(str).apply(lambda x: demoji.replace(x,' '))\n",
    "data12['text'] = data12.text.astype(str).apply(lambda x: demoji.replace(x,' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df2d35-eea7-4693-b02a-f1fcc95c0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "remove = string.punctuation\n",
    "remove = remove.replace(\"@\", \"\") # don't remove \n",
    "remove = remove.replace(\"_\", \"\")\n",
    "remove = remove.replace(\"-\", \"\")\n",
    "pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "#re.sub(pattern, \"\", txt)\n",
    "#data16 = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data16]\n",
    "#df[\"new_column\"] = df['review'].str.replace('[^\\w\\s]','')\n",
    "data8['text'] = data8.text.replace(pattern, ' ',regex=True).astype(str)\n",
    "data9['text'] = data9.text.replace(pattern, ' ',regex=True).astype(str)\n",
    "data10['text'] = data10.text.replace(pattern, ' ',regex=True).astype(str)\n",
    "data11['text'] = data11.text.replace(pattern, ' ',regex=True).astype(str)\n",
    "data12['text'] = data12.text.replace(pattern, ' ',regex=True).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255acf28-de3d-4ae2-a480-8ea966877106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractize(float1):\n",
    "    str1 = str(float1)\n",
    "    lst2 = [word for word in str1.split() if word.startswith('@')]\n",
    "    return lst2\n",
    "\n",
    "data8['ats'] = data8['text'].apply(extractize)\n",
    "data9['ats'] = data9.text.apply(extractize)\n",
    "data10['ats'] = data10.text.apply(extractize)\n",
    "data11['ats'] = data11.text.apply(extractize)\n",
    "data12['ats'] = data12.text.apply(extractize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f962d198-3bfe-48d6-871b-5107e5117245",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp8a = data8.ats.tolist()\n",
    "temp9a = data9.ats.tolist()\n",
    "temp10a = data10.ats.tolist()\n",
    "temp11a = data11.ats.tolist()\n",
    "temp12a = data12.ats.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a3ef1b-e273-41a1-bb19-0e9fdf5cfeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data8['replied-to'].fillna('', inplace = True)\n",
    "data9['replied-to'].fillna('', inplace = True)\n",
    "data10['replied-to'].fillna('', inplace = True)\n",
    "data11['replied-to'].fillna('', inplace = True)\n",
    "data12['replied-to'].fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac8c0db-75f7-49ec-9492-154eb18d56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## optional\n",
    "MT = data8[['ats','replied-to']]\n",
    "AW = data9[['ats','replied-to']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e2ad7-4b3d-4cef-87eb-1f3458a6f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "\n",
    "MT['ats'] = MT.ats.astype(str)\n",
    "AW['ats'] = AW.ats.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f10b9-3480-43ea-a9a5-d7f4ddfe9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "\n",
    "MT['comb'] = MT['ats'] + MT['replied-to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3d8a7-413b-442f-9f39-3bc2e03b7721",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "\n",
    "AW['comb'] = AW['ats'] + AW['replied-to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dba133-35d7-44c4-b765-a9fb7ec9b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "\n",
    "MT['len'] = MT.apply(lambda row: len(row.comb), axis=1)\n",
    "AW['len'] = AW.apply(lambda row: len(row.comb), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a246473-c21d-43f9-aa5d-743ecc3e1d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "\n",
    "MT[MT.len > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1af3e-1c34-46be-981f-47f14e528afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp8b = data8['replied-to'].tolist()\n",
    "temp9b = data9['replied-to'].tolist()\n",
    "temp10b = data10['replied-to'].tolist()\n",
    "temp11b = data11['replied-to'].tolist()\n",
    "temp12b = data12['replied-to'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca26d1-de24-4010-8677-32e936d9ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp8b = [x for x in temp8b if x]\n",
    "temp9b = [x for x in temp9b if x]\n",
    "temp10b = [x for x in temp10b if x]\n",
    "temp11b = [x for x in temp11b if x]\n",
    "temp12b = [x for x in temp12b if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286f1a3-a920-409a-acab-0ecfe84d1376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "temp8b = [literal_eval(x) for x in temp8b]\n",
    "temp9b = [literal_eval(x) for x in temp9b]\n",
    "temp10b = [literal_eval(x) for x in temp10b]\n",
    "temp11b = [literal_eval(x) for x in temp11b]\n",
    "temp12b = [literal_eval(x) for x in temp12b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73075a0a-1f92-40ab-8d6a-810cc4a572d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85b2654-d171-47d4-9ef2-666eb5463a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "MetzTilly = []\n",
    "AWIntergroup = [] \n",
    "Act4AnimalsEU = []\n",
    "LadyFreethinker = []\n",
    "ProtectWldlife = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8027bc-6388-4374-a4b1-224919d738bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "[MetzTilly.extend(x) for x in temp8a]\n",
    "[AWIntergroup.extend(x) for x in temp9a]\n",
    "[Act4AnimalsEU.extend(x) for x in temp10a]\n",
    "[LadyFreethinker.extend(x) for x in temp11a]\n",
    "[ProtectWldlife.extend(x) for x in temp12a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c8917b-4a2e-4abf-9cd1-f6520a4a911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[MetzTilly.extend(x) for x in temp8b]\n",
    "[AWIntergroup.extend(x) for x in temp9b]\n",
    "[Act4AnimalsEU.extend(x) for x in temp10b]\n",
    "[LadyFreethinker.extend(x) for x in temp11b]\n",
    "[ProtectWldlife.extend(x) for x in temp12b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a353dfe-dc92-44e3-811e-89c4148b8d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MetzTilly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855fe3df-5203-4a53-91b1-8f84a2161c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_MetzTilly = list(set(MetzTilly))\n",
    "unique_AWIntergroup = list(set(AWIntergroup))\n",
    "unique_Act4AnimalsEU = list(set(Act4AnimalsEU))\n",
    "unique_LadyFreethinker = list(set(LadyFreethinker))\n",
    "unique_ProtectWldlife = list(set(ProtectWldlife))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34f2c4-545a-418f-ba6b-478587ecee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_MetzTilly = ['@MetzTilly'] * len(unique_MetzTilly)\n",
    "repeat_AWIntergroup = ['@AWIntergroup'] * len(unique_AWIntergroup)\n",
    "repeat_Act4AnimalsEU = ['@Act4AnimalsEU'] * len(unique_Act4AnimalsEU)\n",
    "repeat_LadyFreethinker = ['@LadyFreethinker'] * len(unique_LadyFreethinker)\n",
    "repeat_ProtectWldlife = ['@ProtectWldlife'] * len(unique_ProtectWldlife)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0851636-064e-4b2a-a218-2a369fd31a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_MetzTilly = list(zip(repeat_MetzTilly, unique_MetzTilly))\n",
    "tuple_AWIntergroup = list(zip(repeat_AWIntergroup, unique_AWIntergroup))\n",
    "tuple_Act4AnimalsEU = list(zip(repeat_Act4AnimalsEU, unique_Act4AnimalsEU))\n",
    "tuple_LadyFreethinker = list(zip(repeat_LadyFreethinker, unique_LadyFreethinker))\n",
    "tuple_ProtectWldlife = list(zip(repeat_ProtectWldlife, unique_ProtectWldlife))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2511a69-0fd4-4467-9ce3-9df6d39237fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "#e = [(1,2),(1,3),(2,3)]\n",
    "G = nx.Graph()\n",
    "\n",
    "G.add_edges_from(tuple_MetzTilly)\n",
    "G.add_edges_from(tuple_AWIntergroup)\n",
    "G.add_edges_from(tuple_Act4AnimalsEU)\n",
    "G.add_edges_from(tuple_LadyFreethinker)\n",
    "G.add_edges_from(tuple_ProtectWldlife)\n",
    "\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d542ba7d-96d0-4981-9c80-d96eaed4b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/mayeesha/network-analysis-for-dummies-stackoverflow-data\n",
    "\n",
    "sorted_cliques = sorted(list(nx.find_cliques(G)),key=len)\n",
    "\n",
    "max_clique_nodes = set()\n",
    "\n",
    "for nodelist in sorted_cliques[-4:-1]:\n",
    "    for node in nodelist:\n",
    "        max_clique_nodes.add(node)\n",
    "\n",
    "max_clique = G.subgraph(max_clique_nodes)\n",
    "\n",
    "#print(nx.info(max_clique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb38f3-e256-4f6c-b5e5-71075dbe64d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_graph(G):\n",
    "    nodes = G.nodes()\n",
    "#    color_map = {1:'#f09494', 2:'#eebcbc', 3:'#72bbd0', 4:'#91f0a1', 5:'#629fff', 6:'#bcc2f2',  \n",
    "#             7:'#eebcbc', 8:'#f1f0c0', 9:'#d2ffe7', 10:'#caf3a6', 11:'#ffdf55', 12:'#ef77aa', \n",
    "#             13:'#d6dcff', 14:'#d2f5f0'}\n",
    "#    node_color= [color_map[d['group']] for n,d in G.nodes(data=True)]\n",
    "#    node_size = [d['nodesize']*10 for n,d in G.nodes(data=True)]\n",
    "    pos = nx.drawing.spring_layout(G,k=0.70,iterations=60)\n",
    "    plt.figure()\n",
    "    nx.draw_networkx(G,pos=pos,edge_color='#FFDEA2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b5f3e-dd76-44b3-a36f-bd40a0b95575",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(max_clique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559793b-7c4c-41c5-ba9c-0ab66dd6a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = [node for node,degree in dict(G.degree()).items() if degree < 4]\n",
    "G.remove_nodes_from(remove)\n",
    "list(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a35d809-9545-4755-9acf-02fd5b0716c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check networkx\n",
    "\n",
    "lst2 = [x for x in unique_LadyFreethinker if x in unique_ProtectWldlife]\n",
    "print(len(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76c7c9-efd3-4bfa-9d35-7640e63fe011",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad78b89b-418a-44f4-a572-8c045c8fec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "\n",
    "all = []\n",
    "all.extend(MetzTilly)\n",
    "all.extend(AWIntergroup)\n",
    "all.extend(Act4AnimalsEU)\n",
    "all.extend(LadyFreethinker)\n",
    "all.extend(ProtectWldlife)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c1c20-ad44-4424-893d-cfd7374de1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "unique_list = list(set(all))\n",
    "len(unique_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38c50f-9ea5-4225-bb0a-a377b85f3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('aw_global_csv.csv')\n",
    "compare1 = data.username.tolist()\n",
    "\n",
    "compare2 = list(set(compare1))\n",
    "len(compare2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d28d88-abdf-4987-ae05-7e15034d534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "\n",
    "lst2 = [x for x in compare2 if x in unique_list]\n",
    "print(len(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5acf1f-63ea-403a-a445-155cd39da956",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(MetzTilly), len(AWIntergroup), len(Act4AnimalsEU), len(LadyFreethinker), len(ProtectWldlife))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3281d11-c55f-43bc-8fe6-13f562a1bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "import pandas as pd\n",
    "\n",
    "followers = pd.read_csv('result_followers_AWIntergroup.csv')\n",
    "following = pd.read_csv('result_following_AWIntergroup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4ea31d-0ed6-49c3-bfc6-516090ab39a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "unique_AWIntergroup = [w.replace('@', '') for w in unique_AWIntergroup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf804e-7f9b-43cd-84db-872ea7a6e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "followers = followers.screenName.unique().tolist()\n",
    "following = following.screenName.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f8d0d-45f4-49e9-99e0-2f231d63973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "\n",
    "print(len(unique_AWIntergroup), len(followers), len(following))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee3e43-ee3a-4401-9a8e-d359e1d0cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional\n",
    "\n",
    "lst3 = [x for x in followers if x in unique_AWIntergroup]\n",
    "lst4 = [x for x in following if x in unique_AWIntergroup]\n",
    "print(len(lst3), len(lst4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NitScrap",
   "language": "python",
   "name": "nitscrap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
