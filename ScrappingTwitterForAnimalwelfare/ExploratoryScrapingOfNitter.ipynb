{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667a7636-7014-436f-8731-099b51e048ee",
   "metadata": {},
   "source": [
    "Content\n",
    "1. [Scraping a dataset from Nitter based on the hashtag #animalwelfare](#1.-Scraping-a-dataset-from-Nitter-based-on-the-hashtag-#animalwelfare)\n",
    "2. [Finding unique tweeters and scraping their profile info](#2.-Finding-unique-tweeters-and-scraping-their-profile-info)\n",
    "3. [Basic attributes about tweets and tweeters](#3.-Basic-attributes-about-tweets-and-tweeters)\n",
    "4. [Analyzing manually annotated lists of users](#4.-Analyzing-manually-annotated-lists-of-users)\n",
    "5. [Wordclouds](#5.-Wordclouds)\n",
    "6. [Wordcount](#6.-Wordcount)\n",
    "7. [Boxplots and dotplots](7.-Boxplots-and-dotplots)\n",
    "8. [Hashtags of selected accounts](#8.-Hashtags-of-selected-accounts)\n",
    "9. [Text column cleanup](#9.-Text-column-cleanup)\n",
    "10. [Clustering of tweets using BERT](#10.-Clustering-of-tweets-using-BERT)\n",
    "11. [Automatic categorizing using BERT](#11.-Automatic-categorizing-using-BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7f64db-92f4-4816-8c48-3141d9b3feaf",
   "metadata": {},
   "source": [
    "Main questions behind exploratory analysis:\n",
    "\n",
    "How to analyze the discussion on animalwelfare on twitter through Nitter? How to structure the information regarding animalwelfare on twitter? Do easy fixes exist: can hashtags be used to gain an  overview on the animalwelfare discussion in twitter? Can subtopics be automatically identified? Can discussion participants be automatically classified? How diverse is the community in their information generation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59ce93-513e-40cd-ab35-4754aa5a1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Overview of packages used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9593aa4-dd34-42f7-bec5-f84bed575c84",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe9949-446d-44d9-aae5-55b4641253d6",
   "metadata": {},
   "source": [
    "##1. Scraping a dataset from Nitter based on the hashtag #animalwelfare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b75762e-8bea-44c5-b7b5-705ef5813790",
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialize the scraper\n",
    "\n",
    "import pandas as pd\n",
    "from ntscraper import Nitter\n",
    "\n",
    "scraper = Nitter(log_level=1, skip_instance_check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb50124-2dd3-41a6-a613-86b45577cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### do the actual scraping\n",
    "\n",
    "aw_hash_tweets = scraper.get_tweets(\"animalwelfare\", mode='hashtag', number=5000, since='2023-10-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63124bee-7c25-4898-becf-d870976b5410",
   "metadata": {},
   "outputs": [],
   "source": [
    "### extract necessary info\n",
    "\n",
    "final_tweets = []\n",
    "\n",
    "for tweet in aw_hash_tweets['tweets']:\n",
    "    data = [tweet['link'], tweet['text'], tweet['user']['username'], tweet['user']['profile_id'], tweet['date'], tweet['stats']['retweets'], tweet['stats']['likes'], tweet['stats']['comments'], tweet['stats']['quotes']]\n",
    "    final_tweets.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910bf88-f5db-4ac6-8a9f-5355641fa046",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create pandas dataframe with info\n",
    "\n",
    "data = pd.DataFrame(final_tweets, columns = ['link', 'text', 'username', 'profile_id', 'date', 'retweets', 'likes', 'comments', 'quotes'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c7de8-56c6-4c69-9b0a-adbcd81ad051",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_interactions'] = data['retweets'] + data['likes'] + data['comments'] + data['quotes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212fb042-13cd-40ae-bcd1-1bb27287403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean-up bio's column to make better excel readable\n",
    "\n",
    "data['text'] = data['text'].str.replace(r'\\n',' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71021b13-7d0f-43c5-bc45-a05b86db0ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('aw_global_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3495053f-35df-44b6-ac67-98fb65266a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data again\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('aw_global_csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4a9773-b3a5-4137-86af-5f50f63afc1c",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b27d562-1bb6-4021-afaf-13d57019b31b",
   "metadata": {},
   "source": [
    "##2. Finding unique tweeters and scraping their profile info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc573aa-6157-49c0-86d7-74c7913b4480",
   "metadata": {},
   "outputs": [],
   "source": [
    "###find unique tweeters\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_sub1 = pd.DataFrame(data.groupby('username')['total_interactions'].sum(), columns = ['total_interactions'])\n",
    "data_sub2 = pd.DataFrame(data.groupby('username')['link'].nunique(), columns=['link'])\n",
    "\n",
    "df_tweeters = pd.merge(data_sub2, data_sub1, how='inner', on='username')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518cc2c-66fc-419f-94e4-c3221c6f476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tweeters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c3386-7eec-4a4f-bed1-bb1265c16db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tweeters = df_tweeters.sort_values(by=['total_interactions'])\n",
    "final_tweeters = final_tweeters.tail(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c4ca54-ae31-41c8-b0f4-d57704f91cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_profiles = []\n",
    "\n",
    "#for i in range(len(new3)):\n",
    "#    pr-info = new3.loc[i, \"username\"]\n",
    "\n",
    "for ind in final_tweeters.index:\n",
    "#    pr_info = new7['username'][ind]\n",
    "    pr_info = ind\n",
    "    try:\n",
    "        profile = scraper.get_profile_info(pr_info)\n",
    "    except:\n",
    "        data2 = ['NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN']\n",
    "    try:\n",
    "        data2 = [profile['username'], profile['bio'], profile['joined'], profile['stats']['tweets'], profile['stats']['following'], profile['stats']['followers'], profile['stats']['likes'], profile['stats']['media']]\n",
    "    except:\n",
    "        data2 = ['NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN']\n",
    "    final_profiles.append(data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d03487-adf0-4c2e-a367-7d4725ea91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c805ea-dbe3-438d-b209-a5a7dbb7293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save results\n",
    "\n",
    "data2['bio'] = data2['bio'].str.replace(r'\\n',' ', regex=True)\n",
    "data2.to_csv('aw_users_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfca283-dc5c-48c6-a900-eaa24565abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load results\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data2 = pd.read_csv('aw_users_csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f7419-a38e-42f7-a05a-c51a88f484e2",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f608060-f26a-4f97-ba36-99ec732af785",
   "metadata": {},
   "source": [
    "##3. Basic attributes about tweets and tweeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd51add-9c06-421f-8b24-44eeaf5e7d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565edf7-4046-4b1b-bb45-520eab2d8f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 20 posts with most interactions\n",
    "\n",
    "df_topposts = data.sort_values(by=['total_interactions'])\n",
    "df_topposts.tail(20).to_csv('aw_top20posts_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55865bc-a008-469f-936d-4d6314335a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show how many tweets had interactions\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "A = data.total_interactions.isin([0]).sum()\n",
    "B = 5000 - A\n",
    "\n",
    "y = np.array([A, B])\n",
    "mylabels = [\"Tweets without interactions\", \"Tweets with interactions\"]\n",
    "\n",
    "plt.pie(y, labels = mylabels)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60373fa-6c93-4b50-8f43-006abadf01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many tweets contained the word China, trade, dog, cat, farming\n",
    "\n",
    "china = [s for s in data.text if \"China\" in s]\n",
    "trade = [s for s in data.text if \"trade\" in s]\n",
    "dog = [s for s in data.text if \"dog\" in s]\n",
    "cat = [s for s in data.text if \"cat\" in s]\n",
    "farming = [s for s in data.text if \"farming\" in s]\n",
    "\n",
    "print(len(china), len(trade), len(dog), len(cat), len(farming))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46214408-a312-4044-b60c-49b79d3c0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "### select tweets with \"#animalwelfare\" and \"China\" or \"@China\"\n",
    "\n",
    "China = pd.DataFrame(china, columns=['text'])\n",
    "China.to_csv('chinatweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f879a6-35b6-4981-939d-b3cc03a08d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "### show interactions per tweet\n",
    "\n",
    "hist = data.total_interactions.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f9478-4293-4a6c-af6c-81cdb1597961",
   "metadata": {},
   "outputs": [],
   "source": [
    "### show correlation between likes and retweets\n",
    "\n",
    "data.plot.scatter(x = 'Retweets', y = 'Likes', s = 100, c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86135e88-4131-4e97-b9a7-8a7528dd8d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "### show how many unique users had interactions\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "A = data_sub1.total_interactions.isin([0]).sum()\n",
    "B = 5000 - A\n",
    "\n",
    "y = np.array([A, B])\n",
    "mylabels = [\"Users without interactions\", \"Users with interactions\"]\n",
    "\n",
    "plt.pie(y, labels = mylabels)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd5995-4cfb-4ef6-89c4-490397b3360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### how often did single users tweet\n",
    "\n",
    "hist = data_sub2.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a01e09-aaba-420b-84d6-b71c83584bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### who where the most liked users\n",
    "\n",
    "temp1 = data_sub1.sort_values(by=['total_interactions'])\n",
    "temp1.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f06ca2-33f9-474e-a459-74f7cba624eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### How many interactions did the top 30 users receive\n",
    "\n",
    "C = temp1.total_interactions.tail(30).sum() / temp1.total_interactions.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d643b-ef00-4233-99b2-7e4bf6ad783a",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44e865-a4ac-4101-8b73-a588ec3d1f43",
   "metadata": {},
   "source": [
    "##4. Scrap tweets per user per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacaff95-1738-4e73-9197-f466ca5c7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e59370-0bde-4b0d-8115-0a0bb16c6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get activity of users for a given time period with ntscrapper\n",
    "\n",
    "###\n",
    "final_september = []\n",
    "\n",
    "for ind in data2.username:\n",
    "    user_name = ind\n",
    "    try:\n",
    "        tweets_september = scraper.get_tweets(user_name, mode='user', since='2023-09-01', until='2023-10-01')\n",
    "        for tweet in tweets_september['tweets']:\n",
    "            data = [tweet['link'], tweet['text'], tweet['user']['username'], tweet['user']['profile_id'], tweet['date'], tweet['stats']['retweets'], tweet['stats']['likes'], tweet['stats']['comments'], tweet['stats']['quotes']]\n",
    "            final_september.append(data)\n",
    "    except:\n",
    "        data = ['NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN']\n",
    "        final_september.append(data)\n",
    "        \n",
    "september = pd.DataFrame(final_september, columns = ['link', 'text', 'username', 'profile_id', 'date', 'retweets', 'likes', 'comments', 'quotes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58fccf3-a509-4ea5-be8b-c7650e161b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save\n",
    "\n",
    "september['text'] = september['text'].str.replace(r'\\n',' ', regex=True)\n",
    "september.to_csv('aw_september_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f0ebc5-b678-412f-a90c-0d34d46c999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load\n",
    "\n",
    "september = pd.read_csv('aw_september_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1779f5-661d-4c22-87d1-b159d52cd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how often did single users tweet\n",
    "\n",
    "sep2 = september.groupby('username')['link'].nunique()\n",
    "hist = sep2.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f03fd8-5c4b-412d-8fff-f73f5317b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in data2.username:\n",
    "    user_name = ind\n",
    "    try:\n",
    "        tweets_october = scraper.get_tweets(user_name, mode='user', since='2023-10-01', until='2023-11-01')\n",
    "        for tweet in tweets_october['tweets']:\n",
    "            data = [tweet['link'], tweet['text'], tweet['user']['username'], tweet['user']['profile_id'], tweet['date'], tweet['stats']['retweets'], tweet['stats']['likes'], tweet['stats']['comments'], tweet['stats']['quotes']]\n",
    "            final_october.append(data)\n",
    "    except:\n",
    "        data = ['NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN']\n",
    "        final_october.append(data)\n",
    "        \n",
    "october = pd.DataFrame(final_october, columns = ['link', 'text', 'username', 'profile_id', 'date', 'retweets', 'likes', 'comments', 'quotes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd100adc-3fe8-4600-b447-5154f1662c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save\n",
    "\n",
    "october.to_csv('aw_october_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954a424-ae47-4fd4-a218-4f9a6667e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how often did single users tweet\n",
    "\n",
    "oct2 = october.groupby('username')['link'].nunique()\n",
    "hist = oct2.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a20c0-293a-4736-b9aa-e3180d893955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sep2\n",
    "sep2 = pd.DataFrame(sep2)\n",
    "oct2 = pd.DataFrame(oct2)\n",
    "#new11 = pd.merge(new9, sep2, left_on=[\"col0\", \"col1\"], right_index=True, how=\"right\")\n",
    "temp2 = pd.merge(data2, sep2, how='inner', on='username')\n",
    "temp2.rename(columns={'link': 'september_tweets'}, inplace=True)\n",
    "\n",
    "temp3 = pd.merge(temp2, oct2, how='inner', on='username')\n",
    "temp3.rename(columns={'link': 'october_tweets'}, inplace=True)\n",
    "\n",
    "temp4 = temp3.sort_values('total_interactions')\n",
    "#new9 = new8.tail(400)\n",
    "temp5 = temp4.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec780c2-4366-4bec-ba3e-0a33acbab20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### scatter monthly tweeting activity vs total interactions\n",
    "\n",
    "ax1 = temp4.plot(kind='scatter', x='september_tweets', y='total_interactions', color='r', label=\"september\")    \n",
    "ax2 = temp4.plot(kind='scatter', x='october_tweets', y='total_interactions', color='g', label=\"october\", ax=ax1) \n",
    "\n",
    "ax1.set_xlabel(\"monthly tweets\")\n",
    "ax1.set_ylabel(\"total interactions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a5ceba-c64a-443a-ae34-929309d1472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp6 = data2[['username', 'followers']]\n",
    "temp7 = pd.merge(temp4, temp6, how='left', on='username')\n",
    "temp8 = temp7.sort_values('total_interactions')\n",
    "temp9 = temp8.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8057586f-679b-4ba3-a3b5-bae2a7cda8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef643b-2aad-463d-be1a-0bb780546b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### scatter total_interactions vs followers\n",
    "\n",
    "ax1 = temp8.plot(kind='scatter', x='followers', y='total_interactions', color='b')    \n",
    "\n",
    "ax1.set_xlabel(\"followers\")\n",
    "ax1.set_ylabel(\"total interactions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f0ab58-e259-4fe6-b29f-f9be9762bd96",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea72bb12-a6c1-40f6-bb79-78b057f634c7",
   "metadata": {},
   "source": [
    "##4. Analyzing manually annotated lists of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b78fdb-817f-49bd-9719-98accc76350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd7d05-5849-4269-bcfe-dfd776cb0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated = pd.read_csv('annotated_users_interactions.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5cf0e-7309-416a-b55d-74d30c42165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4ec87-9fa3-4a7a-9e47-70c7097f18e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotated = pd.merge(annotated, data_sub2, how='inner', on='username')\n",
    "df_annotated['ratio'] = df_annotated['total_interactions'] / df_annotated['link']\n",
    "#df_annotated2 = df_annotated.sort_values(by=['ratio'])\n",
    "df_annotated['username'] = df_annotated['username'].str.replace('@', '')\n",
    "col_list = df_annotated.username.values.tolist()\n",
    "print(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aeedbc-328d-4bb3-8712-f9845c051f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "follnum_annotataed = []\n",
    "\n",
    "#for i in range(len(new3)):\n",
    "#    pr-info = new3.loc[i, \"username\"]\n",
    "\n",
    "#for ind in df_annotated.username:\n",
    "#    pr_info = new7['username'][ind]\n",
    "#    pr_info = df_annotated['username'][ind]\n",
    "#    profile = scraper.get_profile_info(pr_info)\n",
    "for i in col_list:\n",
    "    try: \n",
    "        profile = scraper.get_profile_info(i)\n",
    "    except:\n",
    "        data2 = ['NaN', 'NaN', 'NaN', 'NaN', 'NaN']\n",
    "    try:\n",
    "        data2 = [profile['stats']['tweets'], profile['stats']['following'], profile['stats']['followers'], profile['stats']['likes'], profile['stats']['media']]\n",
    "    except:\n",
    "        data2 = ['NaN', 'NaN', 'NaN', 'NaN', 'NaN']\n",
    "    follnum_annotataed.append(data2)\n",
    "\n",
    "data4 = pd.DataFrame(follnum_annotataed, columns = ['tweets', 'following', 'followers', 'likes', 'media'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a099c-f8c3-40a2-9b65-dbeb4622d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data3\n",
    "\n",
    "temp10 = pd.DataFrame(col_list, columns = ['username'])\n",
    "df_annotated2 = pd.merge(temp10, data4, left_index=True, right_index=True)\n",
    "df_annotated3 = pd.merge(df_annotated2, df_annotated, how='inner', on='username') \n",
    "df_annotated3 = df_annotated3.sort_values(by=['ratio'])\n",
    "print(df_annotated3[['username', 'ratio', 'followers', 'tweets']].tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5b26b0-792c-4e65-903d-c3d3ed114acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = df_annotated3.plot(kind='scatter', x='followers', y='ratio', color='r', label=\"followers\")    \n",
    "ax2 = df_annotated3.plot(kind='scatter', x='tweets', y='ratio', color='g', label=\"tweets\", ax=ax1) \n",
    "\n",
    "ax1.set_xlabel(\"number of tweets or of followers\")\n",
    "ax1.set_ylabel(\"ratio interactions / #animalwelfare tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a36d22-5633-4ee0-b83d-25bd1d3102c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = df_annotated3.ratio.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51434d-1748-4be5-ba8d-37f6d3136ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = df_annotated3.tweets.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e749716e-8d38-491f-b75f-3ee85c95f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp11 = df_annotated3.loc[df_annotated3['total_interactions'] <= 3 ]\n",
    "hist = temp11.followers.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ecf069-399e-428c-85e2-0570011d8b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp11[['followers']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec14d23-865d-45f5-971a-2483d713dae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotated3[['tweets']].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0ca30-3606-4a50-95f7-9db1338de5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "follnum_annotated = []\n",
    "\n",
    "#for i in range(len(new3)):\n",
    "#    pr-info = new3.loc[i, \"username\"]\n",
    "\n",
    "#for ind in df_annotated.username:\n",
    "#    pr_info = new7['username'][ind]\n",
    "#    pr_info = df_annotated['username'][ind]\n",
    "#    profile = scraper.get_profile_info(pr_info)\n",
    "for i in col_list:\n",
    "    try: \n",
    "        profile = scraper.get_profile_info(i)\n",
    "    except:\n",
    "        data2 = ['NaN']\n",
    "    try:\n",
    "        data2 = [profile['bio']]\n",
    "    except:\n",
    "        data2 = ['NaN']\n",
    "    follnum_annotated.append(data2)\n",
    "\n",
    "data5 = pd.DataFrame(follnum_annotated, columns = ['bio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6445bb31-21e3-4155-b7df-1284c7f86f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp12 = data[['username', 'profile_id']]\n",
    "temp13 = temp12.drop_duplicates()\n",
    "temp13['username'] = temp13['username'].str.replace('@', '')\n",
    "print(temp13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ce62c1-f075-4aca-ba62-dd6f8d36902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new50 = pd.DataFrame(col_list, columns = ['username'])\n",
    "df_annotated4 = pd.merge(temp10, data5, left_index=True, right_index=True)\n",
    "df_annotated5 = pd.merge(df_annotated3, temp13, how='inner', on='username') \n",
    "df_annotated6 = pd.merge(df_annotated5, df_annotated4, how='inner',on='username') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f8246-a2d6-424c-9a9f-ed9aa458267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotated6.to_csv('annotated_with_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35aac1a-8e6c-460d-83df-748f4a412892",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c884adb-8a66-45d7-bcdb-e56d114e8af1",
   "metadata": {},
   "source": [
    "##5. Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5347789-a26a-46b2-83ea-1b46dbe4723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import json\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd61fe1-297b-42fb-80e1-4c838a2ce620",
   "metadata": {},
   "outputs": [],
   "source": [
    "single = df_annotated4['bio'].to_string(index=False)\n",
    "\n",
    "print(len(single))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602236b0-117c-4e78-b04d-3f9e2287e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to use for category\n",
    "#df_annotated8 = df_annotated7.loc[df_annotated7['Type'] == 'NGO']\n",
    "#single = df_annotated8['bio'].to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e305fa-99c2-49b8-a71f-422308512b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer= WordNetLemmatizer()\n",
    "\n",
    "#extra_stop_filter2 = ['’', \"'s\", \"n't\", 'promotion', 'collected', 'comfortable', 'part', \"'ve\", 'shoe']\n",
    "stop_filters = stopwords.words('english') + list(string.punctuation) \n",
    "#+ extra_stop_filter2\n",
    "\n",
    "best_comf_tokens = [lemmatizer.lemmatize(tokens) for tokens in word_tokenize(single)\\\n",
    "                    if tokens not in stop_filters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd41e4c-9ac7-4875-827a-77c8040f4917",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_comf_words_filtered = single.replace('based', \"\").replace('music', \"\").replace('running', \"\")\n",
    "#best_comf_words_filtered = single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7efc512-52fb-4f1f-bcd6-7e5470a2fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", max_words=2000, width=800, height=400)\n",
    "# generate word cloud\n",
    "wc.generate(best_comf_words_filtered)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Most used Words in annotated users' Bios\",fontsize= 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63431ed7-f4fa-4f75-b269-0682e2b5d971",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5502ef-eae7-4790-90ab-b5facaac5b15",
   "metadata": {},
   "source": [
    "##6. Wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134da0a5-042b-4568-aadd-9e0969a63e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17157348-4d36-4dd6-b012-be13f915b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotated6 = pd.read_csv('annotated_with_info.csv')\n",
    "categories = pd.read_csv('categories.csv', sep=';')\n",
    "categories2 = categories.drop(columns=['total_interactions'])\n",
    "categories2['username'] = categories2['username'].str.replace('@', '')\n",
    "df_annotated7 = pd.merge(df_annotated6, categories2, how='left', on='username')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eb7b21-c896-4950-87ce-bcf862dc91ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "df_annotated7['bio'] = df_annotated7['bio'].astype(str) \n",
    "\n",
    "df_annotated7['bio_without_stopwords'] = df_annotated7['bio'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#print(Act4AnimalsEU)\n",
    "df_annotated7['bio_without_stopwords'] = df_annotated7['bio_without_stopwords'].str.replace('nan','').replace('&','').replace('-','')\n",
    "\n",
    "wl = Counter(\" \".join(df_annotated7[\"bio_without_stopwords\"].str.lower()).split()).most_common(100)\n",
    "\n",
    "#data = np.asarray(my_list)\n",
    "\n",
    "data = np.asarray(wl)\n",
    "a = data.tolist()\n",
    "a = pd.DataFrame(a, columns=['word', 'frequency'])\n",
    "a.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc0319-a814-45c9-b029-2d5238630696",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "df_annotated8['bio'] = df_annotated8['bio'].astype(str) \n",
    "\n",
    "df_annotated8['bio_without_stopwords'] = df_annotated8['bio'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#print(Act4AnimalsEU)\n",
    "df_annotated8['bio_without_stopwords'] = df_annotated8['bio_without_stopwords'].str.replace('nan','').replace('&','').replace('-','')\n",
    "\n",
    "wl = Counter(\" \".join(df_annotated8[\"bio_without_stopwords\"].str.lower()).split()).most_common(100)\n",
    "\n",
    "#data = np.asarray(my_list)\n",
    "\n",
    "data = np.asarray(wl)\n",
    "a = data.tolist()\n",
    "a = pd.DataFrame(a, columns=['word', 'frequency'])\n",
    "a.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f01e5-9ba5-4599-ba86-03466575c290",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c38aef-bcc2-44d1-bd41-058a54179074",
   "metadata": {},
   "source": [
    "##7. Boxplots and dotplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e4e585-d3f3-4184-a0fd-e3a73c4ca744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_cc = pd.read_csv('Boxplot.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca6818-b2c3-499a-9cdc-d3343dbfd2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cc.ratio.isna().sum()\n",
    "df_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aba606-c1fb-4eb7-9e52-de4284367bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import from_levels_and_colors\n",
    "#colours = [mcolors.darkviolet, mcolors.deepskyblue, mcolors.yellow, mcolors.darkgreen, mcolors.hotpink]\n",
    "#colours = ['darkviolet', 'deepskyblue', 'yellow', 'darkgreen', 'hotpink', 'black', 'orange', 'dimgray', 'red', 'paleturquoise', 'mediumspringgreen']\n",
    "colours = {1:'darkviolet', 2:'deepskyblue', 3:'yellow', 4:'darkgreen', 5:'hotpink', 6:'black', 7:'orange', 8:'dimgray', 9:'red', 10:'paleturquoise', 11:'mediumspringgreen'}\n",
    "col_list = df_cc.Coded.values.tolist()\n",
    "converted_colors = [colours[color] for color in col_list]\n",
    "#cmap, norm = from_levels_and_colors([1,2,3,4,5,6,7,8,9,10,11], ['darkviolet', 'deepskyblue', 'yellow', 'darkgreen', 'hotpink', 'black', 'orange', 'dimgray', 'red', 'paleturquoise', 'mediumspringgreen'])\n",
    "#plt.scatter(df_cc.Coded, df_cc.total_interactions, s=200, c=df_cc.Coded, cmap=cmap)\n",
    "plt.scatter(df_cc.Coded, df_cc.total_interactions, c=converted_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ad3b6-2bb1-4b17-8ae4-22a9b7f055b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import from_levels_and_colors\n",
    "#colours = [mcolors.darkviolet, mcolors.deepskyblue, mcolors.yellow, mcolors.darkgreen, mcolors.hotpink]\n",
    "#colours = ['darkviolet', 'deepskyblue', 'yellow', 'darkgreen', 'hotpink', 'black', 'orange', 'dimgray', 'red', 'paleturquoise', 'mediumspringgreen']\n",
    "colours = {1:'darkviolet', 2:'deepskyblue', 3:'yellow', 4:'darkgreen', 5:'hotpink', 6:'black', 7:'orange', 8:'dimgray', 9:'red', 10:'paleturquoise', 11:'mediumspringgreen'}\n",
    "col_list = df_cc.Coded.values.tolist()\n",
    "converted_colors = [colours[color] for color in col_list]\n",
    "#cmap, norm = from_levels_and_colors([1,2,3,4,5,6,7,8,9,10,11], ['darkviolet', 'deepskyblue', 'yellow', 'darkgreen', 'hotpink', 'black', 'orange', 'dimgray', 'red', 'paleturquoise', 'mediumspringgreen'])\n",
    "#plt.scatter(df_cc.Coded, df_cc.ratio, s=200, c=df_cc.Coded, cmap=cmap)\n",
    "plt.scatter(df_cc.Coded, df_cc.ratio, c=converted_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad89a6-dda5-4bb1-965a-ffda2c844528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import from_levels_and_colors\n",
    "#colours = [mcolors.darkviolet, mcolors.deepskyblue, mcolors.yellow, mcolors.darkgreen, mcolors.hotpink]\n",
    "#colours = ['darkviolet', 'deepskyblue', 'yellow', 'darkgreen', 'hotpink', 'black', 'orange', 'dimgray', 'red', 'paleturquoise', 'mediumspringgreen']\n",
    "colours = {1:'darkviolet', 2:'deepskyblue', 3:'yellow', 4:'darkgreen', 5:'hotpink', 6:'black', 7:'orange', 8:'dimgray', 9:'red', 10:'paleturquoise', 11:'mediumspringgreen'}\n",
    "col_list = df_cc.Coded.values.tolist()\n",
    "converted_colors = [colours[color] for color in col_list]\n",
    "#cmap, norm = from_levels_and_colors([1,2,3,4,5,6,7,8,9,10,11], ['darkviolet', 'deepskyblue', 'yellow', 'darkgreen', 'hotpink', 'black', 'orange', 'dimgray', 'red', 'paleturquoise', 'mediumspringgreen'])\n",
    "#plt.scatter(df_cc.Coded, df_cc.ratio, s=200, c=df_cc.Coded, cmap=cmap)\n",
    "plt.scatter(df_cc.Coded, df_cc.followers, c=converted_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf5c45-e2cc-47e6-a1ba-9f64baea3719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import from_levels_and_colors\n",
    "#colours = [mcolors.darkviolet, mcolors.deepskyblue, mcolors.yellow, mcolors.darkgreen, mcolors.hotpink]\n",
    "#colours = ['darkviolet', 'deepskyblue', 'yellow', 'darkgreen', 'hotpink', 'black', 'orange', 'dimgray', 'red', 'paleturquoise', 'mediumspringgreen']\n",
    "colours = {1:'darkviolet', 2:'deepskyblue', 3:'yellow', 4:'darkgreen', 5:'hotpink', 6:'black', 7:'orange', 8:'dimgray', 9:'red', 10:'paleturquoise', 11:'mediumspringgreen'}\n",
    "col_list = df_cc.Coded.values.tolist()\n",
    "converted_colors = [colours[color] for color in col_list]\n",
    "#cmap, norm = from_levels_and_colors([1,2,3,4,5,6,7,8,9,10,11], ['darkviolet', 'deepskyblue', 'yellow', 'darkgreen', 'hotpink', 'black', 'orange', 'dimgray', 'red', 'paleturquoise', 'mediumspringgreen'])\n",
    "#plt.scatter(df_cc.Coded, df_cc.ratio, s=200, c=df_cc.Coded, cmap=cmap)\n",
    "plt.scatter(df_cc.Coded, df_cc.tweets, c=converted_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3b00fe-4ca4-46c6-a1b2-2ec5bc5bcb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_box = df_cc.loc[df_cc['Coded'].isin([2, 3, 6, 7])]\n",
    "bp = df_box.boxplot(column = 'tweets', by='Coded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484c078-c5e8-4d95-823e-5a70cef7a576",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69266271-ba73-4ebf-a1cf-e9e09e8b2e62",
   "metadata": {},
   "source": [
    "##8. Hashtags of selected accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcac47af-c054-4d38-bd91-3199c23202b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MetzTilly = scraper.get_tweets(\"MetzTilly\", mode='user', since='2020-01-01')\n",
    "\n",
    "MetzTilly_tweets = []\n",
    "\n",
    "for tweet in MetzTilly['tweets']:\n",
    "    data = [tweet['link'], tweet['date'], tweet['stats']['retweets'], tweet['stats']['likes'], tweet['stats']['comments'], tweet['stats']['quotes'], tweet['text']]\n",
    "    MetzTilly_tweets.append(data)\n",
    "\n",
    "data8 = pd.DataFrame(MetzTilly_tweets, columns = ['link', 'date', 'retweets', 'likes', 'comments', 'quotes', 'text'])\n",
    "data8.to_csv('MetzTilly_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d63da8-32ee-41c7-981a-5a10671c9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWIntergroup = scraper.get_tweets(\"AWIntergroup\", mode='user', since='2020-01-01')\n",
    "\n",
    "AWIntergroup_tweets = []\n",
    "\n",
    "for tweet in AWIntergroup['tweets']:\n",
    "    data = [tweet['link'], tweet['date'], tweet['stats']['retweets'], tweet['stats']['likes'], tweet['stats']['comments'], tweet['stats']['quotes'], tweet['text']]\n",
    "    AWIntergroup_tweets.append(data)\n",
    "\n",
    "data9 = pd.DataFrame(AWIntergroup_tweets, columns = ['link', 'date', 'retweets', 'likes', 'comments', 'quotes', 'text'])\n",
    "data9.to_csv('AWIntergroup_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cecd7fb-0ac9-4155-bc0d-7da216df62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data8.retweets.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ee42d-e304-4aef-8ceb-b77bbf2c70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data9['hashtags'] = data9.text.str.findall(r'#.*?(?=\\s|$)')\n",
    "data8['hashtags'] = data8.text.str.findall(r'#.*?(?=\\s|$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9066a97-aaae-4659-9eec-839ae291cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### find tweets without hashtags\n",
    "\n",
    "data9[~data9['hashtags'].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367747d-0fad-48aa-aa9f-33c7ea979951",
   "metadata": {},
   "outputs": [],
   "source": [
    "data10 = data8.explode('hashtags')\n",
    "data11 = data9.explode('hashtags')\n",
    "\n",
    "data10[['hashtags']] = data10[['hashtags']].fillna('')\n",
    "data11[['hashtags']] = data11[['hashtags']].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a26bfd-3aa4-492e-ab03-e11aae3cfa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "data10['hashtags'] = data10['hashtags'].astype('string')\n",
    "data11['hashtags'] = data11['hashtags'].astype('string')\n",
    "\n",
    "data10['hashtags'] = data10['hashtags'].str.lower()\n",
    "data11['hashtags'] = data11['hashtags'].str.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "data10['hashtags'] = data10['hashtags'].apply(remove_punctuation)\n",
    "data11['hashtags'] = data11['hashtags'].apply(remove_punctuation)\n",
    "result = data11.dtypes\n",
    "print(data11.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd6f8d-186e-4881-bd00-e66f79561453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "\n",
    "MT = data10.groupby('hashtags')['link'].nunique()\n",
    "MT = pd.DataFrame(MT, columns=['link'])\n",
    "MT = MT.sort_values(by=['link'])\n",
    "print(MT.tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c5b940-e07c-4954-839f-9f5dc439d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AW = data11.groupby('hashtags')['link'].nunique()\n",
    "AW = pd.DataFrame(AW, columns=['link'])\n",
    "AW = AW.sort_values(by=['link'])\n",
    "print(AW.tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5bd524-6812-4cb9-bc41-f4d4118b24f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "data9['text'] = data8['text'].astype(str) \n",
    "\n",
    "data9['text_without_stopwords'] = data9['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#print(Act4AnimalsEU)\n",
    "data9['text_without_stopwords'] = data9['text_without_stopwords'].str.replace('nan','').replace('&','').replace('-','')\n",
    "\n",
    "wl = Counter(\" \".join(data9[\"text_without_stopwords\"].str.lower()).split()).most_common(100)\n",
    "\n",
    "#data = np.asarray(my_list)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data = np.asarray(wl)\n",
    "a = data.tolist()\n",
    "a = pd.DataFrame(a, columns=['word', 'frequency'])\n",
    "a.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa56028-c384-40b6-b0c4-43efc36f3e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "data12 = data10.loc[data10['hashtags'] == 'animalwelfare']\n",
    "data13 = data11.loc[data11['hashtags'] == 'animalwelfare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7820526b-9e58-493f-8deb-fc84b6926c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7f4b5-48d9-4e88-9d1b-5f81ea4ac60e",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180eeb70-ba60-4b2d-abba-0ec4042cf34b",
   "metadata": {},
   "source": [
    "##9. Text column cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e7caa-8094-44be-aa37-1126c0de1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0cc608-7000-40e6-b088-5255179d9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert df here\n",
    "#data8 = XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b5857-9de7-4371-9793-59ae97394c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji\n",
    "datanew = data8.text.astype(str).apply(lambda x: demoji.replace(x,''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2736eb-4407-46af-ae41-75e45ce86e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data16 = datanew.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac06ca-a5e2-46f9-95e2-e5f2712d474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Emails\n",
    "data16 = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data16]\n",
    "\n",
    "# Remove new line characters\n",
    "data16 = [re.sub('\\s+', ' ', sent) for sent in data16]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data16 = [re.sub(\"\\'\", \"\", sent) for sent in data16]\n",
    "\n",
    "print(data16[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ce6f5-a32c-457a-8075-eec2220b6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls\n",
    "\n",
    "import re\n",
    "\n",
    "data16 = [re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', sent) for sent in data16]\n",
    "\n",
    "#URLless_string = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', thestring)\n",
    "print(data16[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda624f-1ab0-4e81-8643-eae3da98c51a",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ecbbb-9bb0-4263-a906-61b9c15f118a",
   "metadata": {},
   "source": [
    "##10. Clustering of tweets using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c44aea-a14d-4a0d-af64-a180d4a9a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "docs = data16\n",
    "model = SentenceTransformer('Nhat1904/Final-32shots-Twitter-Skhead-Train-5epoch')\n",
    "vectorized_docs = model.encode(np.asarray(docs))\n",
    "\n",
    "print(\"Shape:\", vectorized_docs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b18e70-fd73-4a9f-beac-03c98970ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data16, columns=['text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f0425-589c-4655-9eb8-880cee05c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "### elbow analysis\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def graw_elbow_graph(x: np.array, k1: int, k2: int, k3: int):\n",
    "    k_values, inertia_values = [], []\n",
    "    for k in range(k1, k2, k3):\n",
    "        print(\"Processing:\", k)\n",
    "        km = KMeans(n_clusters=k).fit(x)\n",
    "        k_values.append(k)\n",
    "        inertia_values.append(km.inertia_)\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(k_values, inertia_values, 'o')\n",
    "    plt.title('Inertia for each K')\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Inertia')\n",
    "\n",
    "\n",
    "graw_elbow_graph(vectorized_docs, 2, 50, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e732b-2c2d-48dd-912d-c9613d6b4660",
   "metadata": {},
   "outputs": [],
   "source": [
    "## clustering -> needs k\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "df = pd.DataFrame(data16, columns=['text_clean'])\n",
    "\n",
    "def make_clustered_dataframe(x: np.array, k: int) -> pd.DataFrame:\n",
    "    \"\"\" Create a new dataframe with original docs and assigned clusters \"\"\"\n",
    "#    ids = df[\"id\"].values\n",
    "#    user_names = df[\"user_name\"].values\n",
    "    docs = df[\"text_clean\"].values\n",
    "#    tokenized_docs = df[\"text_clean\"].map(text_to_tokens).values\n",
    "    \n",
    "    km = KMeans(n_clusters=k).fit(x)\n",
    "    s_score = silhouette_score(x, km.labels_)\n",
    "    print(f\"K={k}: Silhouette coefficient {s_score:0.2f}, inertia:{km.inertia_}\")\n",
    "    \n",
    "    # Create new DataFrame\n",
    "    data_len = x.shape[0]\n",
    "    df_clusters = pd.DataFrame({\n",
    "#        \"id\": ids[:data_len],\n",
    "#        \"user\": user_names[:data_len],\n",
    "        \"clean_text\": docs[:data_len],\n",
    "#        \"tokens\": tokenized_docs[:data_len],\n",
    "        \"cluster\": km.labels_,\n",
    "    })\n",
    "    return df_clusters\n",
    "\n",
    "\n",
    "#def text_to_tokens(text: str) -> List[str]:\n",
    "#    \"\"\" Generate tokens from the sentence \"\"\"\n",
    "#    # \"this is text\" => ['this', 'is' 'text']\n",
    "#    tokens = word_tokenize(text)  # Get tokens from text\n",
    "#    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "#    return tokens\n",
    "\n",
    "\n",
    "# Make clustered dataframe\n",
    "k = 24\n",
    "df_clusters = make_clustered_dataframe(vectorized_docs, k)\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4febf36-d2ea-4e83-947d-0f162b3fe788",
   "metadata": {},
   "outputs": [],
   "source": [
    "newish = data8.join(df_clusters, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce44dd-7db6-41d0-b6a3-995d6794de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "newish2 = pd.read_csv('Maybe-Yes-No.csv', sep=';')\n",
    "newish2 = newish2.rename(columns={'Name': 'username', 'Status': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc07af0-2e90-4be2-839a-8e922e3292aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "newish3 = newish.merge(newish2, on='username', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8175913-2c87-4a60-84f1-c5665ca8a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes = newish3.loc[newish3['label'] == 'Yes']\n",
    "Maybe = newish3.loc[newish3['label'] == 'Maybe']\n",
    "No = newish3.loc[newish3['label'] == 'No']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7cb6e-93d4-40b1-9caa-10e45c24e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "newish4 = newish3[['label', 'cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9127e192-5f92-4dfa-b567-cdb01b496c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = (\n",
    "    pd.get_dummies(newish4, columns=['cluster'])\n",
    "        .groupby('label', as_index=False)\n",
    "        .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1adcab-7882-4c8c-a0c8-e25570dd5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.set_index('label')\n",
    "df1_transposed = new_df.T # Prepare Data\n",
    "df1_transposed['No'] = df1_transposed[['No']] * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb348b3-ca5c-493b-95b0-91a29ad5640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "# Prepare Data\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 8)\n",
    "\n",
    "#Class\n",
    "AgeClass = df1_transposed.index.tolist()\n",
    "#Chart\n",
    "bar_plot = sns.barplot(x='Yes', y=df1_transposed.index, data=df1_transposed, order=AgeClass, orient='horizontal', dodge=False)\n",
    "bar_plot = sns.barplot(x='No', y=df1_transposed.index, data=df1_transposed, order=AgeClass, orient='horizontal', dodge=False)\n",
    "plt.title(\"Cluster Distribution vs Assigned Label\", fontsize=22)\n",
    "plt.xlabel(\"No/Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3e61e3-bbb4-4d5a-a142-67691f797b92",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f4617-96fb-4881-a99b-88c0e62797ee",
   "metadata": {},
   "source": [
    "##11. Automatic categorizing using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a404c1c-476e-4b33-ace8-ef048fd007ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "path_to_model = './1-BERTMODELS/category_bert_en_pt/'\n",
    "\n",
    "# We are using the sentiment-analysis type (even though our model is not a sentiment analysis model)\n",
    "pipe = pipeline('sentiment-analysis', model=path_to_model, tokenizer=path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abece4e7-02a3-4dc1-be45-516ecb5dfcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "one = pd.read_csv('categories.csv', sep=';')\n",
    "two = pd.read_csv('annotated_with_info-2.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e345be-e138-4900-b214-5547f026a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "one['username'] = one['username'].str.replace('@', '')\n",
    "three = pd.merge(two, one, how='left', on='username')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8eb2fe-a291-410c-a160-be68254a8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed into 9. Text column cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883839f4-fbff-4bf9-9bb8-a7f0b9f7bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed an example input\n",
    "vectorized_docs2 = []\n",
    "for ele in data16:\n",
    "    vectorized_docs2.append(pipe(ele))\n",
    "# output:\n",
    "# [{'label': 'art', 'score': 0.9069588780403137}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aedf25-0286-4762-8f10-6ac9957720f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_modlab = pd.DataFrame.from_records(vectorized_docs2)\n",
    "num = len(vectorized_docs2) + 1\n",
    "lst1 = range(1, num)\n",
    "df_modlab = pd.DataFrame.from_records(vectorized_docs2,index=lst1, columns=['Dic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbce8cd-aaab-4515-aa44-50b47602252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modlab['Dic'].astype(str).map(eval)\n",
    "df_modlab2 = df_modlab['Dic'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f0ba8-8c36-4ef8-9fa7-e0354dcf0a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pie chart (plots value counts in this case)\n",
    "labels = df_modlab2['label'].dropna().unique()\n",
    "actual_values = df_modlab2['label'].value_counts(dropna=True)\n",
    "\n",
    "#choose your colors\n",
    "#colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99','#fffd55']\n",
    " \n",
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "# To denote actual values instead of percentages as labels in the pie chart, reformat autopct\n",
    "values=df_modlab2['label'].value_counts(dropna=True)\n",
    "plt.pie(actual_values, autopct= lambda x: '{:.0f}'.format(x*values.sum()/100), startangle=90)\n",
    "\n",
    "\n",
    "#draw circle (this example creates a donut)\n",
    "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle\n",
    "ax1.axis('equal') \n",
    "\n",
    "# A separate legend with labels (drawn to the bottom left of the pie in this case) \n",
    "plt.legend(labels, bbox_to_anchor = (0.1, .3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NitScrap",
   "language": "python",
   "name": "nitscrap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
